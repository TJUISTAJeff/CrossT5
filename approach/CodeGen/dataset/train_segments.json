{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.embedding_size = config.embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.embedding_size = config.embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        past_key_values_length=0,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        if input_ids is None and inputs_embeds is None:\n            raise ValueError(\"Need to provide either `input_ids` or `input_embeds`.\")\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n        self.output_attentions = config.output_attentions\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n        self.attention_dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n        self.output_attentions = config.output_attentions\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n        self.attention_dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\n        self.output_dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n        self.output_attentions = config.output_attentions\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def call(\n        self,\n        input_tensor: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(input_tensor)[0]\n        mixed_query_layer = self.query(inputs=input_tensor)\n        mixed_key_layer = self.key(inputs=input_tensor)\n        mixed_value_layer = self.value(inputs=input_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)"}
{"text": "    def call(\n        self,\n        input_tensor: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(input_tensor)[0]\n        mixed_query_layer = self.query(inputs=input_tensor)\n        mixed_key_layer = self.key(inputs=input_tensor)\n        mixed_value_layer = self.value(inputs=input_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFAlbertModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.attention_dropout(inputs=attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = tf.multiply(attention_probs, head_mask)\n\n        context_layer = tf.matmul(attention_probs, value_layer)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.attention = TFAlbertAttention(config, name=\"attention\")\n        self.ffn = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.activation = get_tf_activation(config.hidden_act)\n        else:\n            self.activation = config.hidden_act\n\n        self.ffn_output = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n        )\n        self.full_layer_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.attention = TFAlbertAttention(config, name=\"attention\")\n        self.ffn = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.attention = TFAlbertAttention(config, name=\"attention\")\n        self.ffn = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.activation = get_tf_activation(config.hidden_act)\n        else:\n            self.activation = config.hidden_act\n\n        self.ffn_output = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.attention = TFAlbertAttention(config, name=\"attention\")\n        self.ffn = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.activation = get_tf_activation(config.hidden_act)\n        else:\n            self.activation = config.hidden_act\n\n        self.ffn_output = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n        )\n        self.full_layer_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.num_hidden_layers = config.num_hidden_layers\n        self.num_hidden_groups = config.num_hidden_groups\n        # Number of layers in a hidden group\n        self.layers_per_group = int(config.num_hidden_layers / config.num_hidden_groups)\n        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"embedding_hidden_mapping_in\",\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, input_embeddings: tf.keras.layers.Layer, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.embedding_size = config.embedding_size\n        self.dense = tf.keras.layers.Dense(\n            config.embedding_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.albert = TFAlbertMainLayer(config, add_pooling_layer=False, name=\"albert\")\n        classifier_dropout_prob = (\n            config.classifier_dropout_prob\n            if config.classifier_dropout_prob is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=classifier_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.albert = TFAlbertMainLayer(config, add_pooling_layer=False, name=\"albert\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len)) * LARGE_NEGATIVE\n    mask_cond = tf.range(shape_list(mask)[-1])"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len)) * LARGE_NEGATIVE\n    mask_cond = tf.range(shape_list(mask)[-1])\n\n    mask = tf.where(mask_cond < tf.reshape(mask_cond + 1, (shape_list(mask)[-1], 1)), 0.0, mask)\n\n    if past_key_values_length > 0:\n        mask = tf.concat([tf.zeros((tgt_len, past_key_values_length)), mask], axis=-1)"}
{"text": "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_shape[:2]\n\n        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBartAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBartAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBartAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n\n        self.embed_tokens = embed_tokens\n        self.embed_positions = TFBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.layers = [TFBartEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.embed_tokens = embed_tokens\n        self.layerdrop = config.decoder_layerdrop\n        self.embed_positions = TFBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n        self.layers = [TFBartDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        past_key_values_length=0,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        if input_ids is None and inputs_embeds is None:\n            raise ValueError(\"Need to provide either `input_ids` or `input_embeds`.\")\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)"}
{"text": "layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)"}
{"text": "f.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "cores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(inputs=attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = tf.multiply(attention_probs, head_mask)"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: BertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = get_tf_activation(config.hidden_act)\n        else:\n            self.transform_act_fn = config.hidden_act\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=classifier_dropout)"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=classifier_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.bert = TFBertMainLayer(config, add_pooling_layer=False, name=\"bert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=classifier_dropout)"}
{"text": "    def __init__(self, config: BertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.bert = TFBertMainLayer(config, add_pooling_layer=False, name=\"bert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=classifier_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len))"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len)) * LARGE_NEGATIVE\n    mask_cond = tf.range(shape_list(mask)[-1])"}
{"text": "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_shape[:2]\n\n        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: BlenderbotConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len))"}
{"text": "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_shape[:2]\n\n        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotSmallAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotSmallAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotSmallAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFBlenderbotSmallAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n\n        self.embed_tokens = embed_tokens\n        self.embed_positions = TFBlenderbotSmallLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.layers = [TFBlenderbotSmallEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.embed_tokens = embed_tokens\n        self.layerdrop = config.decoder_layerdrop\n        self.embed_positions = TFBlenderbotSmallLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n        self.layers = [TFBlenderbotSmallDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)\n\n        paddings = tf.constant(\n            [\n                [\n                    0,\n                    0,\n                ],\n                [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)],\n                [0, 0],\n            ]\n        )\n\n        conv_out_layer = self.conv_out_layer(hidden_states)\n        conv_out_layer = tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)\n\n        paddings = tf.constant(\n            [\n                [\n                    0,\n                    0,\n                ],\n                [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)],\n                [0, 0],\n            ]\n        )\n\n        conv_out_layer = self.conv_out_layer(hidden_states)\n        conv_out_layer = tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])\n        conv_out_layer = tf.pad(conv_out_layer, paddings, \"CONSTANT\")\n\n        unfold_conv_out_layer = tf.stack(\n            [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)\n\n        paddings = tf.constant(\n            [\n                [\n                    0,\n                    0,\n                ],\n                [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)],\n                [0, 0],\n            ]\n        )\n\n        conv_out_layer = self.conv_out_layer(hidden_states)\n        conv_out_layer = tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])\n        conv_out_layer = tf.pad(conv_out_layer, paddings, \"CONSTANT\")\n\n        unfold_conv_out_layer = tf.stack(\n            [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])"}
{"text": "ply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)\n\n        paddings = tf.constant(\n            [\n                [\n                    0,\n                    0,\n                ],\n                [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)],\n                [0, 0],\n            ]\n        )\n\n        conv_out_layer = self.conv_out_layer(hidden_states)\n        conv_out_layer = tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])\n        conv_out_layer = tf.pad(conv_out_layer, paddings, \"CONSTANT\")\n\n        unfold_conv_out_layer = tf.stack(\n            [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)"}
{"text": "  [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        value_layer = tf.reshape(\n            mixed_value_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size]\n        )\n        value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n        context_layer = tf.matmul(attention_probs, value_layer)\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n\n        conv_out = tf.reshape(conv_out_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size])"}
{"text": "ize, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        value_layer = tf.reshape(\n            mixed_value_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size]\n        )\n        value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n        context_layer = tf.matmul(attention_probs, value_layer)\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n\n        conv_out = tf.reshape(conv_out_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size])\n        context_layer = tf.concat([context_layer, conv_out], 2)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def call(self, hidden_states):\n        batch_size = shape_list(hidden_states)[0]\n        x = tf.reshape(hidden_states, [-1, self.num_groups, self.group_in_dim])"}
{"text": "    def call(self, hidden_states):\n        batch_size = shape_list(hidden_states)[0]\n        x = tf.reshape(hidden_states, [-1, self.num_groups, self.group_in_dim])\n        x = tf.matmul(x, tf.transpose(self.kernel, [2, 1, 0]))\n        x = tf.transpose(x, [1, 0, 2])\n        x = tf.reshape(x, [batch_size, -1, self.output_size])"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.num_groups == 1:\n            self.dense = tf.keras.layers.Dense(\n                config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n            )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.num_groups == 1:\n            self.dense = tf.keras.layers.Dense(\n                config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n            )\n        else:\n            self.dense = GroupedLinearLayer(\n                config.intermediate_size,\n                config.hidden_size,\n                num_groups=config.num_groups,\n                kernel_initializer=get_initializer(config.initializer_range),\n                name=\"dense\",\n            )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.num_groups == 1:\n            self.dense = tf.keras.layers.Dense(\n                config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n            )\n        else:\n            self.dense = GroupedLinearLayer(\n                config.intermediate_size,\n                config.hidden_size,\n                num_groups=config.num_groups,\n                kernel_initializer=get_initializer(config.initializer_range),\n                name=\"dense\",\n            )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)\n        self.out_proj = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.convbert = TFConvBertMainLayer(config, name=\"convbert\")\n        self.sequence_summary = TFSequenceSummary(\n            config, initializer_range=config.initializer_range, name=\"sequence_summary\"\n        )\n        self.classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.convbert = TFConvBertMainLayer(config, name=\"convbert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.convbert = TFConvBertMainLayer(config, name=\"convbert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "def scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=None):\n    # calculate attention\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    dk = tf.cast(shape_list(k)[-1], dtype=matmul_qk.dtype)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += tf.cast(mask * -1e4, dtype=scaled_attention_logits.dtype)\n\n    if attention_mask is not None:\n        # Apply the attention mask\n        attention_mask = tf.cast(attention_mask, dtype=scaled_attention_logits.dtype)\n        scaled_attention_logits = scaled_attention_logits + attention_mask\n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)"}
{"text": "def scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=None):\n    # calculate attention\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    dk = tf.cast(shape_list(k)[-1], dtype=matmul_qk.dtype)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += tf.cast(mask * -1e4, dtype=scaled_attention_logits.dtype)\n\n    if attention_mask is not None:\n        # Apply the attention mask\n        attention_mask = tf.cast(attention_mask, dtype=scaled_attention_logits.dtype)\n        scaled_attention_logits = scaled_attention_logits + attention_mask\n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    # Mask heads if we want to\n    if head_mask is not None:\n        attention_weights = attention_weights * head_mask\n\n    output = tf.matmul(attention_weights, v)"}
{"text": "    def __init__(self, d_model_size, num_heads, output_attentions=False, **kwargs):\n        super().__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model_size = d_model_size\n        self.output_attentions = output_attentions\n\n        self.depth = int(d_model_size / self.num_heads)\n\n        self.Wq = tf.keras.layers.Dense(d_model_size, name=\"Wq\")"}
{"text": "    def __init__(self, d_model_size, num_heads, output_attentions=False, **kwargs):\n        super().__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model_size = d_model_size\n        self.output_attentions = output_attentions\n\n        self.depth = int(d_model_size / self.num_heads)\n\n        self.Wq = tf.keras.layers.Dense(d_model_size, name=\"Wq\")\n        self.Wk = tf.keras.layers.Dense(d_model_size, name=\"Wk\")\n        self.Wv = tf.keras.layers.Dense(d_model_size, name=\"Wv\")"}
{"text": "    def __init__(self, d_model_size, num_heads, output_attentions=False, **kwargs):\n        super().__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model_size = d_model_size\n        self.output_attentions = output_attentions\n\n        self.depth = int(d_model_size / self.num_heads)\n\n        self.Wq = tf.keras.layers.Dense(d_model_size, name=\"Wq\")\n        self.Wk = tf.keras.layers.Dense(d_model_size, name=\"Wk\")\n        self.Wv = tf.keras.layers.Dense(d_model_size, name=\"Wv\")\n\n        self.dense = tf.keras.layers.Dense(d_model_size, name=\"dense\")"}
{"text": "    def call(self, v, k, q, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n        batch_size = shape_list(q)[0]\n\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, batch_size)\n        k = self.split_into_heads(k, batch_size)\n        v = self.split_into_heads(v, batch_size)\n\n        if layer_past is not None:\n            past_key, past_value = tf.unstack(layer_past, axis=0)\n            k = tf.concat((past_key, k), axis=-2)\n            v = tf.concat((past_value, v), axis=-2)"}
{"text": "    def call(self, v, k, q, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n        batch_size = shape_list(q)[0]\n\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, batch_size)\n        k = self.split_into_heads(k, batch_size)\n        v = self.split_into_heads(v, batch_size)\n\n        if layer_past is not None:\n            past_key, past_value = tf.unstack(layer_past, axis=0)\n            k = tf.concat((past_key, k), axis=-2)\n            v = tf.concat((past_value, v), axis=-2)\n\n        if use_cache:\n            present = tf.stack((k, v), axis=0)"}
{"text": "    def __init__(\n        self, d_model_size, num_heads, dff, rate=0.1, layer_norm_epsilon=1e-6, output_attentions=False, **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.output_attentions = output_attentions\n\n        self.multi_head_attention = TFMultiHeadAttention(\n            d_model_size, num_heads, output_attentions=self.output_attentions, name=\"multi_head_attention\"\n        )\n        self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm1\")"}
{"text": "    def __init__(\n        self, d_model_size, num_heads, dff, rate=0.1, layer_norm_epsilon=1e-6, output_attentions=False, **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.output_attentions = output_attentions\n\n        self.multi_head_attention = TFMultiHeadAttention(\n            d_model_size, num_heads, output_attentions=self.output_attentions, name=\"multi_head_attention\"\n        )\n        self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm1\")\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm2\")"}
{"text": "    def __init__(\n        self, d_model_size, num_heads, dff, rate=0.1, layer_norm_epsilon=1e-6, output_attentions=False, **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.output_attentions = output_attentions\n\n        self.multi_head_attention = TFMultiHeadAttention(\n            d_model_size, num_heads, output_attentions=self.output_attentions, name=\"multi_head_attention\"\n        )\n        self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm1\")\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm2\")\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)"}
{"text": "    def __init__(\n        self, d_model_size, num_heads, dff, rate=0.1, layer_norm_epsilon=1e-6, output_attentions=False, **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.output_attentions = output_attentions\n\n        self.multi_head_attention = TFMultiHeadAttention(\n            d_model_size, num_heads, output_attentions=self.output_attentions, name=\"multi_head_attention\"\n        )\n        self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm1\")\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm2\")\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n            use_bias=False,\n        )"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.pooler_hidden_size, name=\"dense\")"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.hidden_size, name=\"dense\")"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "def build_relative_position(query_size, key_size):\n    \"\"\"\n    Build relative position according to the query and key\n\n    We assume the absolute position of query :math:`P_q` is range from (0, query_size) and the absolute position of key\n    :math:`P_k` is range from (0, key_size), The relative positions from query to key is :math:`R_{q \\\\rightarrow k} =\n    P_q - P_k`\n\n    Args:\n        query_size (int): the length of query\n        key_size (int): the length of key\n\n    Return:\n        :obj:`tf.Tensor`: A tensor with shape [1, query_size, key_size]\n\n    \"\"\"\n    q_ids = tf.range(query_size, dtype=tf.int32)"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.in_proj = tf.keras.layers.Dense(\n            self.all_head_size * 3,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"in_proj\",\n            use_bias=False,\n        )"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.in_proj = tf.keras.layers.Dense(\n            self.all_head_size * 3,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"in_proj\",\n            use_bias=False,\n        )\n        self.pos_att_type = config.pos_att_type if config.pos_att_type is not None else []\n\n        self.relative_attention = getattr(config, \"relative_attention\", False)\n        self.talking_head = getattr(config, \"talking_head\", False)\n\n        if self.talking_head:\n            self.head_logits_proj = tf.keras.layers.Dense(\n                self.num_attention_heads,\n                kernel_initializer=get_initializer(config.initializer_range),\n                name=\"head_logits_proj\",\n                use_bias=False,\n            )"}
{"text": "_head_size\n        self.in_proj = tf.keras.layers.Dense(\n            self.all_head_size * 3,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"in_proj\",\n            use_bias=False,\n        )\n        self.pos_att_type = config.pos_att_type if config.pos_att_type is not None else []\n\n        self.relative_attention = getattr(config, \"relative_attention\", False)\n        self.talking_head = getattr(config, \"talking_head\", False)\n\n        if self.talking_head:\n            self.head_logits_proj = tf.keras.layers.Dense(\n                self.num_attention_heads,\n                kernel_initializer=get_initializer(config.initializer_range),\n                name=\"head_logits_proj\",\n                use_bias=False,\n            )\n            self.head_weights_proj = tf.keras.layers.Dense(\n                self.num_attention_heads,\n                kernel_initializer=get_initializer(config.initializer_range),\n                name=\"head_weights_proj\",\n                use_bias=False,\n            )\n\n        self.softmax = TFDebertaXSoftmax(axis=-1)\n\n        if self.relative_attention:\n            self.max_relative_positions = getattr(config, \"max_relative_positions\", -1)\n            if self.max_relative_positions < 1:\n                self.max_relative_positions = config.max_position_embeddings\n            self.pos_dropout = TFDebertaStableDropout(config.hidden_dropout_prob, name=\"pos_dropout\")\n            if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                self.pos_proj = tf.keras.layers.Dense(\n                    self.all_head_size,\n                    kernel_initializer=get_initializer(config.initializer_range),\n                    name=\"pos_proj\",\n                    use_bias=False,\n                )\n            if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                self.pos_q_proj = tf.keras.layers.Dense(\n                    self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"pos_q_proj\"\n                )"}
{"text": "            def linear(w, b, x):\n                return tf.cond(\n                    b is not None,\n                    lambda: tf.matmul(x, w, transpose_b=True)"}
{"text": "            def linear(w, b, x):\n                return tf.cond(\n                    b is not None,\n                    lambda: tf.matmul(x, w, transpose_b=True) + tf.transpose(b),\n                    lambda: tf.matmul(x, w, transpose_b=True)"}
{"text": "ath:`2 \\\\times\n                \\\\text{max_relative_positions}`, `hidden_size`].\n\n\n        \"\"\"\n        if query_states is None:\n            qp = self.in_proj(hidden_states)  # .split(self.all_head_size, dim=-1)\n            query_layer, key_layer, value_layer = tf.split(\n                self.transpose_for_scores(qp), num_or_size_splits=3, axis=-1\n            )\n        else:\n\n            def linear(w, b, x):\n                return tf.cond(\n                    b is not None,\n                    lambda: tf.matmul(x, w, transpose_b=True) + tf.transpose(b),\n                    lambda: tf.matmul(x, w, transpose_b=True),\n                )\n\n            ws = tf.split(\n                tf.transpose(self.in_proj.weight[0]), num_or_size_splits=self.num_attention_heads * 3, axis=0\n            )\n            qkvw = tf.TensorArray(dtype=tf.float32, size=3)\n            for k in tf.range(3):\n                qkvw_inside = tf.TensorArray(dtype=tf.float32, size=self.num_attention_heads)\n                for i in tf.range(self.num_attention_heads):\n                    qkvw_inside = qkvw_inside.write(i, ws[i * 3 + k])\n                qkvw = qkvw.write(k, qkvw_inside.concat())\n            qkvb = [None] * 3\n\n            q = linear(qkvw[0], qkvb[0], query_states)\n            k = linear(qkvw[1], qkvb[1], hidden_states)\n            v = linear(qkvw[2], qkvb[2], hidden_states)\n            query_layer = self.transpose_for_scores(q)\n            key_layer = self.transpose_for_scores(k)\n            value_layer = self.transpose_for_scores(v)\n\n        query_layer = query_layer + self.transpose_for_scores(self.q_bias[None, None, :])\n        value_layer = value_layer + self.transpose_for_scores(self.v_bias[None, None, :])\n\n        rel_att = None\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        scale_factor = 1 + len(self.pos_att_type)\n        scale = math.sqrt(shape_list(query_layer)[-1] * scale_factor)\n        query_layer = query_layer / scale\n\n        attention_scores = tf.matmul(query_layer, key_layer)"}
{"text": " tf.TensorArray(dtype=tf.float32, size=self.num_attention_heads)\n                for i in tf.range(self.num_attention_heads):\n                    qkvw_inside = qkvw_inside.write(i, ws[i * 3 + k])\n                qkvw = qkvw.write(k, qkvw_inside.concat())\n            qkvb = [None] * 3\n\n            q = linear(qkvw[0], qkvb[0], query_states)\n            k = linear(qkvw[1], qkvb[1], hidden_states)\n            v = linear(qkvw[2], qkvb[2], hidden_states)\n            query_layer = self.transpose_for_scores(q)\n            key_layer = self.transpose_for_scores(k)\n            value_layer = self.transpose_for_scores(v)\n\n        query_layer = query_layer + self.transpose_for_scores(self.q_bias[None, None, :])\n        value_layer = value_layer + self.transpose_for_scores(self.v_bias[None, None, :])\n\n        rel_att = None\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        scale_factor = 1 + len(self.pos_att_type)\n        scale = math.sqrt(shape_list(query_layer)[-1] * scale_factor)\n        query_layer = query_layer / scale\n\n        attention_scores = tf.matmul(query_layer, key_layer)\n        if self.relative_attention:\n            rel_embeddings = self.pos_dropout(rel_embeddings, training=training)\n            rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\n\n        if rel_att is not None:\n            attention_scores = attention_scores + rel_att\n\n        if self.talking_head:\n            attention_scores = tf.transpose(\n                self.head_logits_proj(tf.transpose(attention_scores, [0, 2, 3, 1])), [0, 3, 1, 2]\n            )\n\n        attention_probs = self.softmax(attention_scores, attention_mask)\n        attention_probs = self.dropout(attention_probs, training=training)\n        if self.talking_head:\n            attention_probs = tf.transpose(\n                self.head_weights_proj(tf.transpose(attention_probs, [0, 2, 3, 1])), [0, 3, 1, 2]\n            )\n\n        context_layer = tf.matmul(attention_probs, value_layer)"}
{"text": "    def disentangled_att_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor):\n\n        if relative_pos is None:\n            q = shape_list(query_layer)[-2]\n            relative_pos = build_relative_position(q, shape_list(key_layer)[-2])\n        shape_list_pos = shape_list(relative_pos)\n        if len(shape_list_pos) == 2:\n            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n        elif len(shape_list_pos) == 3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bxhxqxk\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = tf.cast(\n            tf.minimum(\n                tf.maximum(shape_list(query_layer)[-2], shape_list(key_layer)[-2]), self.max_relative_positions\n            ),\n            tf.int64,\n        )\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.max_relative_positions - att_span : self.max_relative_positions + att_span, :], 0\n        )\n        if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_key_layer = self.pos_proj(rel_embeddings)\n            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer = self.pos_q_proj(rel_embeddings)\n            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            c2p_att = tf.matmul(query_layer, tf.transpose(pos_key_layer, [0, 1, 3, 2]))"}
{"text": "    def disentangled_att_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor):\n\n        if relative_pos is None:\n            q = shape_list(query_layer)[-2]\n            relative_pos = build_relative_position(q, shape_list(key_layer)[-2])\n        shape_list_pos = shape_list(relative_pos)\n        if len(shape_list_pos) == 2:\n            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n        elif len(shape_list_pos) == 3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bxhxqxk\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = tf.cast(\n            tf.minimum(\n                tf.maximum(shape_list(query_layer)[-2], shape_list(key_layer)[-2]), self.max_relative_positions\n            ),\n            tf.int64,\n        )\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.max_relative_positions - att_span : self.max_relative_positions + att_span, :], 0\n        )\n        if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_key_layer = self.pos_proj(rel_embeddings)\n            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer = self.pos_q_proj(rel_embeddings)\n            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            c2p_att = tf.matmul(query_layer, tf.transpose(pos_key_layer, [0, 1, 3, 2]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)"}
{"text": "st_pos) == 2:\n            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n        elif len(shape_list_pos) == 3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bxhxqxk\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = tf.cast(\n            tf.minimum(\n                tf.maximum(shape_list(query_layer)[-2], shape_list(key_layer)[-2]), self.max_relative_positions\n            ),\n            tf.int64,\n        )\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.max_relative_positions - att_span : self.max_relative_positions + att_span, :], 0\n        )\n        if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_key_layer = self.pos_proj(rel_embeddings)\n            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer = self.pos_q_proj(rel_embeddings)\n            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            c2p_att = tf.matmul(query_layer, tf.transpose(pos_key_layer, [0, 1, 3, 2]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)\n            c2p_att = torch_gather(c2p_att, c2p_dynamic_expand(c2p_pos, query_layer, relative_pos), -1)\n            score += c2p_att\n\n        # position->content\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer /= tf.math.sqrt(tf.cast(shape_list(pos_query_layer)[-1] * scale_factor, dtype=tf.float32))\n            if shape_list(query_layer)[-2] != shape_list(key_layer)[-2]:\n                r_pos = build_relative_position(shape_list(key_layer)[-2], shape_list(key_layer)[-2])\n            else:\n                r_pos = relative_pos\n            p2c_pos = tf.clip_by_value(-r_pos + att_span, 0, att_span * 2 - 1)"}
{"text": "3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bxhxqxk\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = tf.cast(\n            tf.minimum(\n                tf.maximum(shape_list(query_layer)[-2], shape_list(key_layer)[-2]), self.max_relative_positions\n            ),\n            tf.int64,\n        )\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.max_relative_positions - att_span : self.max_relative_positions + att_span, :], 0\n        )\n        if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_key_layer = self.pos_proj(rel_embeddings)\n            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer = self.pos_q_proj(rel_embeddings)\n            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            c2p_att = tf.matmul(query_layer, tf.transpose(pos_key_layer, [0, 1, 3, 2]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)\n            c2p_att = torch_gather(c2p_att, c2p_dynamic_expand(c2p_pos, query_layer, relative_pos), -1)\n            score += c2p_att\n\n        # position->content\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            pos_query_layer /= tf.math.sqrt(tf.cast(shape_list(pos_query_layer)[-1] * scale_factor, dtype=tf.float32))\n            if shape_list(query_layer)[-2] != shape_list(key_layer)[-2]:\n                r_pos = build_relative_position(shape_list(key_layer)[-2], shape_list(key_layer)[-2])\n            else:\n                r_pos = relative_pos\n            p2c_pos = tf.clip_by_value(-r_pos + att_span, 0, att_span * 2 - 1)\n\n        if \"p2c\" in self.pos_att_type:\n            p2c_att = tf.matmul(key_layer, tf.transpose(pos_query_layer, [0, 1, 3, 2]))"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        mask: tf.Tensor = None,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def call(\n        self,\n        input_ids: Optional[TFModelInputType] = None,\n        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=input_shape, value=1)"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: DebertaConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = get_tf_activation(config.hidden_act)\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: DebertaConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaMainLayer(config, name=\"deberta\")\n        self.pooler = TFDebertaContextPooler(config, name=\"pooler\")\n\n        drop_out = getattr(config, \"cls_dropout\", None)\n        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n        self.dropout = TFDebertaStableDropout(drop_out, name=\"cls_dropout\")\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "    def __init__(self, config: DebertaConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaMainLayer(config, name=\"deberta\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: DebertaConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaMainLayer(config, name=\"deberta\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.pooler_hidden_size, name=\"dense\")"}
{"text": "    def call(self, inputs: tf.Tensor, mask: tf.Tensor):\n\n        rmask = tf.logical_not(tf.cast(mask, tf.bool))\n        output = tf.where(rmask, float(\"-inf\"), inputs)\n        output = tf.nn.softmax(output, self.axis)"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.hidden_size, name=\"dense\")"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads\n        _attention_head_size = config.hidden_size // config.num_attention_heads\n        self.attention_head_size = getattr(config, \"attention_head_size\", _attention_head_size)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query_proj\",\n            use_bias=True,\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads\n        _attention_head_size = config.hidden_size // config.num_attention_heads\n        self.attention_head_size = getattr(config, \"attention_head_size\", _attention_head_size)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query_proj\",\n            use_bias=True,\n        )\n        self.key_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"key_proj\",\n            use_bias=True,\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n        self.num_attention_heads = config.num_attention_heads\n        _attention_head_size = config.hidden_size // config.num_attention_heads\n        self.attention_head_size = getattr(config, \"attention_head_size\", _attention_head_size)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query_proj\",\n            use_bias=True,\n        )\n        self.key_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"key_proj\",\n            use_bias=True,\n        )\n        self.value_proj = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"value_proj\",\n            use_bias=True,\n        )"}
{"text": "             Input states to the module usually the output from previous layer, it will be the Q,K and V in\n                `Attention(Q,K,V)`\n\n            attention_mask (:obj:`tf.Tensor`):\n                An attention mask matrix of shape [`B`, `N`, `N`] where `B` is the batch size, `N` is the maximum\n                sequence length in which element [i,j] = `1` means the `i` th token in the input can attend to the `j`\n                th token.\n\n            return_att (:obj:`bool`, optional):\n                Whether return the attention matrix.\n\n            query_states (:obj:`tf.Tensor`, optional):\n                The `Q` state in `Attention(Q,K,V)`.\n\n            relative_pos (:obj:`tf.Tensor`):\n                The relative position encoding between the tokens in the sequence. It's of shape [`B`, `N`, `N`] with\n                values ranging in [`-max_relative_positions`, `max_relative_positions`].\n\n            rel_embeddings (:obj:`tf.Tensor`):\n                The embedding of relative distances. It's a tensor of shape [:math:`2 \\\\times\n                \\\\text{max_relative_positions}`, `hidden_size`].\n\n\n        \"\"\"\n        if query_states is None:\n            query_states = hidden_states\n        query_layer = self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads)\n        key_layer = self.transpose_for_scores(self.key_proj(hidden_states), self.num_attention_heads)\n        value_layer = self.transpose_for_scores(self.value_proj(hidden_states), self.num_attention_heads)\n\n        rel_att = None\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        scale_factor = 1\n        if \"c2p\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2c\" in self.pos_att_type:\n            scale_factor += 1\n        if \"p2p\" in self.pos_att_type:\n            scale_factor += 1\n        scale = tf.math.sqrt(tf.cast(shape_list(query_layer)[-1] * scale_factor, tf.float32))\n        attention_scores = tf.matmul(query_layer, tf.transpose(key_layer, [0, 2, 1]))"}
{"text": "            relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)\n        elif len(shape_list_pos) == 3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bsz x height x query x key\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = self.pos_ebd_size\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.pos_ebd_size - att_span : self.pos_ebd_size + att_span, :], 0\n        )\n        if self.share_att_key:\n            pos_query_layer = tf.tile(\n                self.transpose_for_scores(self.query_proj(rel_embeddings), self.num_attention_heads),\n                [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n            )\n            pos_key_layer = tf.tile(\n                self.transpose_for_scores(self.key_proj(rel_embeddings), self.num_attention_heads),\n                [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n            )\n        else:\n            if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_key_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_key_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n            if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_query_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_query_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_key_layer)[-1] * scale_factor, tf.float32))\n            c2p_att = tf.matmul(query_layer, (pos_key_layer.transpose, [0, 2, 1]))"}
{"text": " elif len(shape_list_pos) == 3:\n            relative_pos = tf.expand_dims(relative_pos, 1)\n        # bsz x height x query x key\n        elif len(shape_list_pos) != 4:\n            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")\n\n        att_span = self.pos_ebd_size\n        rel_embeddings = tf.expand_dims(\n            rel_embeddings[self.pos_ebd_size - att_span : self.pos_ebd_size + att_span, :], 0\n        )\n        if self.share_att_key:\n            pos_query_layer = tf.tile(\n                self.transpose_for_scores(self.query_proj(rel_embeddings), self.num_attention_heads),\n                [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n            )\n            pos_key_layer = tf.tile(\n                self.transpose_for_scores(self.key_proj(rel_embeddings), self.num_attention_heads),\n                [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n            )\n        else:\n            if \"c2p\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_key_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_key_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n            if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_query_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_query_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_key_layer)[-1] * scale_factor, tf.float32))\n            c2p_att = tf.matmul(query_layer, (pos_key_layer.transpose, [0, 2, 1]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)"}
{"text": "tile(\n                    self.transpose_for_scores(self.pos_key_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n            if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_query_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_query_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_key_layer)[-1] * scale_factor, tf.float32))\n            c2p_att = tf.matmul(query_layer, (pos_key_layer.transpose, [0, 2, 1]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)\n            c2p_att = take_along_axis(\n                c2p_att,\n                tf.broadcast_to(\n                    tf.squeeze(c2p_pos, 0),\n                    [shape_list(query_layer)[0], shape_list(query_layer)[1], shape_list(relative_pos)[-1]],\n                ),\n                -1,\n            )\n            score += c2p_att / scale\n\n        # position->content\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_query_layer)[-1] * scale_factor, tf.float32))\n            if shape_list(key_layer)[-2] != shape_list(query_layer)[-2]:\n                r_pos = build_relative_position(\n                    shape_list(key_layer)[-2],\n                    shape_list(key_layer)[-2],\n                    bucket_size=self.position_buckets,\n                    max_position=self.max_relative_positions,\n                )\n                r_pos = tf.expand_dims(r_pos, 0)\n            else:\n                r_pos = relative_pos\n\n            p2c_pos = tf.clip_by_value(-r_pos + att_span, 0, att_span * 2 - 1)"}
{"text": "            [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n            if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n                pos_query_layer = tf.tile(\n                    self.transpose_for_scores(self.pos_query_proj(rel_embeddings), self.num_attention_heads),\n                    [shape_list(query_layer)[0] // self.num_attention_heads, 1, 1],\n                )  # .split(self.all_head_size, dim=-1)\n\n        score = 0\n        # content->position\n        if \"c2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_key_layer)[-1] * scale_factor, tf.float32))\n            c2p_att = tf.matmul(query_layer, (pos_key_layer.transpose, [0, 2, 1]))\n            c2p_pos = tf.clip_by_value(relative_pos + att_span, 0, att_span * 2 - 1)\n            c2p_att = take_along_axis(\n                c2p_att,\n                tf.broadcast_to(\n                    tf.squeeze(c2p_pos, 0),\n                    [shape_list(query_layer)[0], shape_list(query_layer)[1], shape_list(relative_pos)[-1]],\n                ),\n                -1,\n            )\n            score += c2p_att / scale\n\n        # position->content\n        if \"p2c\" in self.pos_att_type or \"p2p\" in self.pos_att_type:\n            scale = tf.math.sqrt(tf.cast(shape_list(pos_query_layer)[-1] * scale_factor, tf.float32))\n            if shape_list(key_layer)[-2] != shape_list(query_layer)[-2]:\n                r_pos = build_relative_position(\n                    shape_list(key_layer)[-2],\n                    shape_list(key_layer)[-2],\n                    bucket_size=self.position_buckets,\n                    max_position=self.max_relative_positions,\n                )\n                r_pos = tf.expand_dims(r_pos, 0)\n            else:\n                r_pos = relative_pos\n\n            p2c_pos = tf.clip_by_value(-r_pos + att_span, 0, att_span * 2 - 1)\n\n        if \"p2c\" in self.pos_att_type:\n            p2c_att = tf.matmul(key_layer, transpose(pos_query_layer, [0, 2, 1]))"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.embedding_size = getattr(config, \"embedding_size\", config.hidden_size)\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.position_biased_input = getattr(config, \"position_biased_input\", True)\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        if self.embedding_size != config.hidden_size:\n            self.embed_proj = tf.keras.layers.Dense(config.hidden_size, bias=False)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        mask: tf.Tensor = None,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = get_tf_activation(config.hidden_act)\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: DebertaV2Config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaV2MainLayer(config, name=\"deberta\")\n        self.pooler = TFDebertaV2ContextPooler(config, name=\"pooler\")\n\n        drop_out = getattr(config, \"cls_dropout\", None)\n        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n        self.dropout = TFDebertaV2StableDropout(drop_out, name=\"cls_dropout\")\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaV2MainLayer(config, name=\"deberta\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: DebertaV2Config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaV2MainLayer(config, name=\"deberta\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config: DebertaV2Config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaV2MainLayer(config, name=\"deberta\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.vocab_size = config.vocab_size\n        self.dim = config.dim\n        self.initializer_range = config.initializer_range\n        self.max_position_embeddings = config.max_position_embeddings\n\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.dropout)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = tf.keras.layers.Dropout(config.attention_dropout)\n        self.output_attentions = config.output_attentions\n\n        assert self.dim % self.n_heads == 0, f\"Hidden size {self.dim} not dividable by number of heads {self.n_heads}\"\n\n        self.q_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"q_lin\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = tf.keras.layers.Dropout(config.attention_dropout)\n        self.output_attentions = config.output_attentions\n\n        assert self.dim % self.n_heads == 0, f\"Hidden size {self.dim} not dividable by number of heads {self.n_heads}\"\n\n        self.q_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"q_lin\"\n        )\n        self.k_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"k_lin\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = tf.keras.layers.Dropout(config.attention_dropout)\n        self.output_attentions = config.output_attentions\n\n        assert self.dim % self.n_heads == 0, f\"Hidden size {self.dim} not dividable by number of heads {self.n_heads}\"\n\n        self.q_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"q_lin\"\n        )\n        self.k_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"k_lin\"\n        )\n        self.v_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"v_lin\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = tf.keras.layers.Dropout(config.attention_dropout)\n        self.output_attentions = config.output_attentions\n\n        assert self.dim % self.n_heads == 0, f\"Hidden size {self.dim} not dividable by number of heads {self.n_heads}\"\n\n        self.q_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"q_lin\"\n        )\n        self.k_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"k_lin\"\n        )\n        self.v_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"v_lin\"\n        )\n        self.out_lin = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"out_lin\"\n        )"}
{"text": "lf, query, key, value, mask, head_mask, output_attentions, training=False):\n        \"\"\"\n        Parameters:\n            query: tf.Tensor(bs, seq_length, dim)\n            key: tf.Tensor(bs, seq_length, dim)\n            value: tf.Tensor(bs, seq_length, dim)\n            mask: tf.Tensor(bs, seq_length)\n\n        Returns:\n            weights: tf.Tensor(bs, n_heads, seq_length, seq_length) Attention weights context: tf.Tensor(bs,\n            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n        \"\"\"\n        bs, q_length, dim = shape_list(query)\n        k_length = shape_list(key)[1]\n        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n        # assert key.size() == value.size()\n        dim_per_head = tf.math.divide(self.dim, self.n_heads)\n        dim_per_head = tf.cast(dim_per_head, dtype=tf.int32)\n        mask_reshape = [bs, 1, 1, k_length]\n\n        def shape(x):\n            \"\"\"separate heads\"\"\"\n            return tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"group heads\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n        q = tf.cast(q, dtype=tf.float32)\n        q = tf.multiply(q, tf.math.rsqrt(tf.cast(dim_per_head, dtype=tf.float32)))\n        k = tf.cast(k, dtype=q.dtype)\n        scores = tf.matmul(q, k, transpose_b=True)  # (bs, n_heads, q_length, k_length)\n        mask = tf.reshape(mask, mask_reshape)  # (bs, n_heads, qlen, klen)\n        # scores.masked_fill_(mask, -float('inf'))            # (bs, n_heads, q_length, k_length)\n\n        mask = tf.cast(mask, dtype=scores.dtype)\n        scores = scores - 1e30 * (1.0 - mask)\n        weights = tf.nn.softmax(scores, axis=-1)"}
{"text": "sk: tf.Tensor(bs, seq_length)\n\n        Returns:\n            weights: tf.Tensor(bs, n_heads, seq_length, seq_length) Attention weights context: tf.Tensor(bs,\n            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n        \"\"\"\n        bs, q_length, dim = shape_list(query)\n        k_length = shape_list(key)[1]\n        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n        # assert key.size() == value.size()\n        dim_per_head = tf.math.divide(self.dim, self.n_heads)\n        dim_per_head = tf.cast(dim_per_head, dtype=tf.int32)\n        mask_reshape = [bs, 1, 1, k_length]\n\n        def shape(x):\n            \"\"\"separate heads\"\"\"\n            return tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"group heads\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n        q = tf.cast(q, dtype=tf.float32)\n        q = tf.multiply(q, tf.math.rsqrt(tf.cast(dim_per_head, dtype=tf.float32)))\n        k = tf.cast(k, dtype=q.dtype)\n        scores = tf.matmul(q, k, transpose_b=True)  # (bs, n_heads, q_length, k_length)\n        mask = tf.reshape(mask, mask_reshape)  # (bs, n_heads, qlen, klen)\n        # scores.masked_fill_(mask, -float('inf'))            # (bs, n_heads, q_length, k_length)\n\n        mask = tf.cast(mask, dtype=scores.dtype)\n        scores = scores - 1e30 * (1.0 - mask)\n        weights = tf.nn.softmax(scores, axis=-1)  # (bs, n_heads, qlen, klen)\n        weights = self.dropout(weights, training=training)  # (bs, n_heads, qlen, klen)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            weights = weights * head_mask\n\n        context = tf.matmul(weights, v)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = tf.keras.layers.Dropout(config.dropout)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.lin1 = tf.keras.layers.Dense(\n            config.hidden_dim, kernel_initializer=get_initializer(config.initializer_range), name=\"lin1\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.lin1 = tf.keras.layers.Dense(\n            config.hidden_dim, kernel_initializer=get_initializer(config.initializer_range), name=\"lin1\"\n        )\n        self.lin2 = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"lin2\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.hidden_dim = config.hidden_dim\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation = config.activation\n        self.output_attentions = config.output_attentions\n\n        assert (\n            config.dim % config.n_heads == 0\n        ), f\"Hidden size {config.dim} not dividable by number of heads {config.n_heads}\"\n\n        self.attention = TFMultiHeadSelfAttention(config, name=\"attention\")\n        self.sa_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"sa_layer_norm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.hidden_dim = config.hidden_dim\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation = config.activation\n        self.output_attentions = config.output_attentions\n\n        assert (\n            config.dim % config.n_heads == 0\n        ), f\"Hidden size {config.dim} not dividable by number of heads {config.n_heads}\"\n\n        self.attention = TFMultiHeadSelfAttention(config, name=\"attention\")\n        self.sa_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"sa_layer_norm\")\n\n        self.ffn = TFFFN(config, name=\"ffn\")\n        self.output_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"output_layer_norm\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.vocab_size = config.vocab_size\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.vocab_transform = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"vocab_transform\"\n        )\n        self.act = get_tf_activation(\"gelu\")\n        self.vocab_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"vocab_layer_norm\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.pre_classifier = tf.keras.layers.Dense(\n            config.dim,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"relu\",\n            name=\"pre_classifier\",\n        )\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.pre_classifier = tf.keras.layers.Dense(\n            config.dim,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"relu\",\n            name=\"pre_classifier\",\n        )\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )\n        self.dropout = tf.keras.layers.Dropout(config.seq_classif_dropout)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )\n        assert config.num_labels == 2, f\"Incorrect number of labels {config.num_labels} instead of 2\"\n        self.dropout = tf.keras.layers.Dropout(config.qa_dropout)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.dropout = tf.keras.layers.Dropout(config.seq_classif_dropout)\n        self.pre_classifier = tf.keras.layers.Dense(\n            config.dim,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"relu\",\n            name=\"pre_classifier\",\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.dropout = tf.keras.layers.Dropout(config.seq_classif_dropout)"}
{"text": "    def __init__(self, config: DPRConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        # resolve name conflict with TFBertMainLayer instead of TFBertModel\n        self.bert_model = TFBertMainLayer(config, name=\"bert_model\")\n        self.config = config\n\n        assert self.config.hidden_size > 0, \"Encoder hidden_size can't be zero\"\n        self.projection_dim = config.projection_dim\n        if self.projection_dim > 0:\n            self.encode_proj = tf.keras.layers.Dense(\n                config.projection_dim, kernel_initializer=get_initializer(config.initializer_range), name=\"encode_proj\"\n            )"}
{"text": "    def __init__(self, config: DPRConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.encoder = TFDPREncoderLayer(config, name=\"encoder\")\n\n        self.qa_outputs = tf.keras.layers.Dense(\n            2, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "e,\n        attention_mask: Optional[tf.Tensor] = None,\n        token_type_ids: Optional[tf.Tensor] = None,\n        inputs_embeds: Optional[tf.Tensor] = None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFDPRContextEncoderOutput, Tuple[tf.Tensor, ...]]:\n        r\"\"\"\n        Return:\n\n        Examples::\n\n            >>> from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer\n            >>> tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n            >>> model = TFDPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', from_pt=True)\n            >>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors='tf')[\"input_ids\"]\n            >>> embeddings = model(input_ids).pooler_output\n        \"\"\"\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = (\n                tf.ones(input_shape, dtype=tf.dtypes.int32)\n                "}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.embedding_size = config.embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.embedding_size = config.embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)"}
{"text": "layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)"}
{"text": "ranspose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFElectraModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: ElectraConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(config.hidden_size, name=\"dense\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(config.hidden_size, name=\"dense\")\n        self.dense_prediction = tf.keras.layers.Dense(1, name=\"dense_prediction\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "   encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if not self.config.is_decoder:\n            inputs[\"use_cache\"] = False\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n\n        if inputs[\"past_key_values\"] is None:\n            past_key_values_length = 0\n            inputs[\"past_key_values\"] = [None] * len(self.encoder.layer)\n        else:\n            past_key_values_length = shape_list(inputs[\"past_key_values\"][0][0])[-2]\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=(batch_size, seq_length + past_key_values_length), value=1)"}
{"text": ",\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if not self.config.is_decoder:\n            inputs[\"use_cache\"] = False\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n\n        if inputs[\"past_key_values\"] is None:\n            past_key_values_length = 0\n            inputs[\"past_key_values\"] = [None] * len(self.encoder.layer)\n        else:\n            past_key_values_length = shape_list(inputs[\"past_key_values\"][0][0])[-2]\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=(batch_size, seq_length + past_key_values_length), value=1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        classifier_dropout = (\n            config.classifhidden_dropout_probier_dropout\n            if config.classifier_dropout is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(config, **kwargs)\n\n        self.electra = TFElectraMainLayer(config, name=\"electra\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(config, **kwargs)\n\n        self.electra = TFElectraMainLayer(config, name=\"electra\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.electra = TFElectraMainLayer(config, name=\"electra\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.electra = TFElectraMainLayer(config, name=\"electra\")\n        self.sequence_summary = TFSequenceSummary(\n            config, initializer_range=config.initializer_range, name=\"sequence_summary\"\n        )\n        self.classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.initializer_range = config.initializer_range\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.initializer_range = config.initializer_range\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout)"}
{"text": "    def __init__(self, config):\n        self.d_model = config.d_model\n        self.attention_type = config.attention_type\n        self.num_blocks = config.num_blocks\n        self.separate_cls = config.separate_cls\n        self.truncate_seq = config.truncate_seq\n        self.pool_q_only = config.pool_q_only\n        self.pooling_type = config.pooling_type\n\n        self.sin_dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)\n            psi = tf.concat([cos_embed, sin_embed], axis=-1)\n            pi = tf.concat([cos_embed_d, cos_embed_d], axis=-1)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)\n            psi = tf.concat([cos_embed, sin_embed], axis=-1)\n            pi = tf.concat([cos_embed_d, cos_embed_d], axis=-1)\n            omega = tf.concat([-sin_embed, cos_embed], axis=-1)"}
{"text": "    def get_position_embeds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)\n            psi = tf.concat([cos_embed, sin_embed], axis=-1)\n            pi = tf.concat([cos_embed_d, cos_embed_d], axis=-1)\n            omega = tf.concat([-sin_embed, cos_embed], axis=-1)\n            return (phi, pi, psi, omega)\n        else:\n            # Notations from the paper, appending A.2.1, final formula.\n            # We need to create and return all the possible vectors R for all blocks and shifts.\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)"}
{"text": "      For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)\n            psi = tf.concat([cos_embed, sin_embed], axis=-1)\n            pi = tf.concat([cos_embed_d, cos_embed_d], axis=-1)\n            omega = tf.concat([-sin_embed, cos_embed], axis=-1)\n            return (phi, pi, psi, omega)\n        else:\n            # Notations from the paper, appending A.2.1, final formula.\n            # We need to create and return all the possible vectors R for all blocks and shifts.\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            # Maximum relative positions for the first input\n            rel_pos_id = tf.range(-seq_len * 2, seq_len * 2, 1.0)\n            zero_offset = seq_len * tf.constant(2)\n            sinusoid = tf.einsum(\"i,d->id\", rel_pos_id, inv_freq)\n            sin_embed = self.sin_dropout(tf.sin(sinusoid), training=training)\n            cos_embed = self.cos_dropout(tf.cos(sinusoid), training=training)\n            pos_embed = tf.concat([sin_embed, cos_embed], axis=-1)\n\n            pos = tf.range(0, seq_len)"}
{"text": "    def stride_pool(self, tensor, axis):\n        \"\"\"\n        Perform pooling by stride slicing the tensor along the given axis.\n        \"\"\"\n        if tensor is None:\n            return None\n\n        # Do the stride pool recursively if axis is a list or a tuple of ints.\n        if isinstance(axis, (list, tuple)):\n            for ax in axis:\n                tensor = self.stride_pool(tensor, ax)\n            return tensor\n\n        # Do the stride pool recursively if tensor is a list or tuple of tensors.\n        if isinstance(tensor, (tuple, list)):\n            return type(tensor)(self.stride_pool(x, axis) for x in tensor)\n\n        # Deal with negative axis\n        axis %= len(shape_list(tensor))\n\n        axis_slice = slice(None, -1, 2) if self.separate_cls and self.truncate_seq else slice(None, None, 2)\n        enc_slice = [slice(None)] * axis + [axis_slice]\n        if self.separate_cls:\n            cls_slice = [slice(None)] * axis + [slice(None, 1)]\n            tensor = tf.concat([tensor[cls_slice], tensor], axis)"}
{"text": "    def pool_tensor(self, tensor, mode=\"mean\", stride=2):\n        \"\"\"Apply 1D pooling to a tensor of size [B x T (x H)].\"\"\"\n        if tensor is None:\n            return None\n\n        # Do the pool recursively if tensor is a list or tuple of tensors.\n        if isinstance(tensor, (tuple, list)):\n            return type(tensor)(self.pool_tensor(tensor, mode=mode, stride=stride) for x in tensor)\n\n        if self.separate_cls:\n            suffix = tensor[:, :-1] if self.truncate_seq else tensor\n            tensor = tf.concat([tensor[:, :1], suffix], axis=1)"}
{"text": "    def pool_tensor(self, tensor, mode=\"mean\", stride=2):\n        \"\"\"Apply 1D pooling to a tensor of size [B x T (x H)].\"\"\"\n        if tensor is None:\n            return None\n\n        # Do the pool recursively if tensor is a list or tuple of tensors.\n        if isinstance(tensor, (tuple, list)):\n            return type(tensor)(self.pool_tensor(tensor, mode=mode, stride=stride) for x in tensor)\n\n        if self.separate_cls:\n            suffix = tensor[:, :-1] if self.truncate_seq else tensor\n            tensor = tf.concat([tensor[:, :1], suffix], axis=1)\n\n        ndim = len(shape_list(tensor))\n        if ndim == 2:\n            tensor = tensor[:, :, None]\n\n        if mode == \"mean\":\n            tensor = tf.nn.avg_pool1d(tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")"}
{"text": "    def pool_tensor(self, tensor, mode=\"mean\", stride=2):\n        \"\"\"Apply 1D pooling to a tensor of size [B x T (x H)].\"\"\"\n        if tensor is None:\n            return None\n\n        # Do the pool recursively if tensor is a list or tuple of tensors.\n        if isinstance(tensor, (tuple, list)):\n            return type(tensor)(self.pool_tensor(tensor, mode=mode, stride=stride) for x in tensor)\n\n        if self.separate_cls:\n            suffix = tensor[:, :-1] if self.truncate_seq else tensor\n            tensor = tf.concat([tensor[:, :1], suffix], axis=1)\n\n        ndim = len(shape_list(tensor))\n        if ndim == 2:\n            tensor = tensor[:, :, None]\n\n        if mode == \"mean\":\n            tensor = tf.nn.avg_pool1d(tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")\n        elif mode == \"max\":\n            tensor = tf.nn.max_pool1d(tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")"}
{"text": "    def pool_tensor(self, tensor, mode=\"mean\", stride=2):\n        \"\"\"Apply 1D pooling to a tensor of size [B x T (x H)].\"\"\"\n        if tensor is None:\n            return None\n\n        # Do the pool recursively if tensor is a list or tuple of tensors.\n        if isinstance(tensor, (tuple, list)):\n            return type(tensor)(self.pool_tensor(tensor, mode=mode, stride=stride) for x in tensor)\n\n        if self.separate_cls:\n            suffix = tensor[:, :-1] if self.truncate_seq else tensor\n            tensor = tf.concat([tensor[:, :1], suffix], axis=1)\n\n        ndim = len(shape_list(tensor))\n        if ndim == 2:\n            tensor = tensor[:, :, None]\n\n        if mode == \"mean\":\n            tensor = tf.nn.avg_pool1d(tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")\n        elif mode == \"max\":\n            tensor = tf.nn.max_pool1d(tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")\n        elif mode == \"min\":\n            tensor = -tf.nn.max_pool1d(-tensor, stride, strides=stride, data_format=\"NWC\", padding=\"SAME\")"}
{"text": "def _relative_shift_gather(positional_attn, context_len, shift):\n    batch_size, n_head, seq_len, max_rel_len = shape_list(positional_attn)\n    # max_rel_len = 2 * context_len + shift -1 is the numbers of possible relative positions i-j\n\n    # What's next is the same as doing the following gather in PyTorch, which might be clearer code but less efficient.\n    # idxs = context_len + torch.arange(0, context_len).unsqueeze(0) - torch.arange(0, seq_len).unsqueeze(1)\n    # # matrix of context_len + i-j\n    # return positional_attn.gather(3, idxs.expand([batch_size, n_head, context_len, context_len]))\n\n    positional_attn = tf.reshape(positional_attn, [batch_size, n_head, max_rel_len, seq_len])"}
{"text": "def _relative_shift_gather(positional_attn, context_len, shift):\n    batch_size, n_head, seq_len, max_rel_len = shape_list(positional_attn)\n    # max_rel_len = 2 * context_len + shift -1 is the numbers of possible relative positions i-j\n\n    # What's next is the same as doing the following gather in PyTorch, which might be clearer code but less efficient.\n    # idxs = context_len + torch.arange(0, context_len).unsqueeze(0) - torch.arange(0, seq_len).unsqueeze(1)\n    # # matrix of context_len + i-j\n    # return positional_attn.gather(3, idxs.expand([batch_size, n_head, context_len, context_len]))\n\n    positional_attn = tf.reshape(positional_attn, [batch_size, n_head, max_rel_len, seq_len])\n    positional_attn = positional_attn[:, :, shift:, :]\n    positional_attn = tf.reshape(positional_attn, [batch_size, n_head, seq_len, max_rel_len - shift])"}
{"text": "    def __init__(self, config, block_index, **kwargs):\n        super().__init__(**kwargs)\n        self.attention_type = config.attention_type\n        self.n_head = n_head = config.n_head\n        self.d_head = d_head = config.d_head\n        self.d_model = d_model = config.d_model\n        self.initializer_range = config.initializer_range\n        self.block_index = block_index\n\n        self.hidden_dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.attention_dropout = tf.keras.layers.Dropout(config.attention_dropout)"}
{"text": "    def __init__(self, config, block_index, **kwargs):\n        super().__init__(**kwargs)\n        self.attention_type = config.attention_type\n        self.n_head = n_head = config.n_head\n        self.d_head = d_head = config.d_head\n        self.d_model = d_model = config.d_model\n        self.initializer_range = config.initializer_range\n        self.block_index = block_index\n\n        self.hidden_dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.attention_dropout = tf.keras.layers.Dropout(config.attention_dropout)\n\n        initializer = get_initializer(config.initializer_range)\n\n        self.q_head = tf.keras.layers.Dense(\n            n_head * d_head, use_bias=False, kernel_initializer=initializer, name=\"q_head\"\n        )"}
{"text": "    def __init__(self, config, block_index, **kwargs):\n        super().__init__(**kwargs)\n        self.attention_type = config.attention_type\n        self.n_head = n_head = config.n_head\n        self.d_head = d_head = config.d_head\n        self.d_model = d_model = config.d_model\n        self.initializer_range = config.initializer_range\n        self.block_index = block_index\n\n        self.hidden_dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.attention_dropout = tf.keras.layers.Dropout(config.attention_dropout)\n\n        initializer = get_initializer(config.initializer_range)\n\n        self.q_head = tf.keras.layers.Dense(\n            n_head * d_head, use_bias=False, kernel_initializer=initializer, name=\"q_head\"\n        )\n        self.k_head = tf.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name=\"k_head\")"}
{"text": "    def __init__(self, config, block_index, **kwargs):\n        super().__init__(**kwargs)\n        self.attention_type = config.attention_type\n        self.n_head = n_head = config.n_head\n        self.d_head = d_head = config.d_head\n        self.d_model = d_model = config.d_model\n        self.initializer_range = config.initializer_range\n        self.block_index = block_index\n\n        self.hidden_dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.attention_dropout = tf.keras.layers.Dropout(config.attention_dropout)\n\n        initializer = get_initializer(config.initializer_range)\n\n        self.q_head = tf.keras.layers.Dense(\n            n_head * d_head, use_bias=False, kernel_initializer=initializer, name=\"q_head\"\n        )\n        self.k_head = tf.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name=\"k_head\")\n        self.v_head = tf.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name=\"v_head\")\n\n        self.post_proj = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, name=\"post_proj\")"}
{"text": "    def __init__(self, config, block_index, **kwargs):\n        super().__init__(**kwargs)\n        self.attention_type = config.attention_type\n        self.n_head = n_head = config.n_head\n        self.d_head = d_head = config.d_head\n        self.d_model = d_model = config.d_model\n        self.initializer_range = config.initializer_range\n        self.block_index = block_index\n\n        self.hidden_dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.attention_dropout = tf.keras.layers.Dropout(config.attention_dropout)\n\n        initializer = get_initializer(config.initializer_range)\n\n        self.q_head = tf.keras.layers.Dense(\n            n_head * d_head, use_bias=False, kernel_initializer=initializer, name=\"q_head\"\n        )\n        self.k_head = tf.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name=\"k_head\")\n        self.v_head = tf.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name=\"v_head\")\n\n        self.post_proj = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, name=\"post_proj\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def relative_positional_attention(self, position_embeds, q_head, context_len, cls_mask=None):\n        \"\"\"Relative attention score for the positional encodings\"\"\"\n        # q_head has shape batch_size x sea_len x n_head x d_head\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula (https://arxiv.org/abs/2006.03236)\n            # phi and pi have shape seq_len x d_model, psi and omega have shape context_len x d_model\n            phi, pi, psi, omega = position_embeds\n            # Shape n_head x d_head\n            u = self.r_r_bias * self.scale\n            # Shape d_model x n_head x d_head\n            w_r = self.r_kernel\n\n            # Shape batch_size x sea_len x n_head x d_model\n            q_r_attention = tf.einsum(\"binh,dnh->bind\", q_head + u, w_r)\n            q_r_attention_1 = q_r_attention * phi[:, None]\n            q_r_attention_2 = q_r_attention * pi[:, None]\n\n            # Shape batch_size x n_head x seq_len x context_len\n            positional_attn = tf.einsum(\"bind,jd->bnij\", q_r_attention_1, psi)"}
{"text": "    def relative_positional_attention(self, position_embeds, q_head, context_len, cls_mask=None):\n        \"\"\"Relative attention score for the positional encodings\"\"\"\n        # q_head has shape batch_size x sea_len x n_head x d_head\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula (https://arxiv.org/abs/2006.03236)\n            # phi and pi have shape seq_len x d_model, psi and omega have shape context_len x d_model\n            phi, pi, psi, omega = position_embeds\n            # Shape n_head x d_head\n            u = self.r_r_bias * self.scale\n            # Shape d_model x n_head x d_head\n            w_r = self.r_kernel\n\n            # Shape batch_size x sea_len x n_head x d_model\n            q_r_attention = tf.einsum(\"binh,dnh->bind\", q_head + u, w_r)\n            q_r_attention_1 = q_r_attention * phi[:, None]\n            q_r_attention_2 = q_r_attention * pi[:, None]\n\n            # Shape batch_size x n_head x seq_len x context_len\n            positional_attn = tf.einsum(\"bind,jd->bnij\", q_r_attention_1, psi)\n        else:\n            # Notations from the paper, appending A.2.1, final formula (https://arxiv.org/abs/2006.03236)\n            # Grab the proper positional encoding, shape max_rel_len x d_model\n            if shape_list(q_head)[1] != context_len:\n                shift = 2\n                r = position_embeds[self.block_index][1]\n            else:\n                shift = 1\n                r = position_embeds[self.block_index][0]\n            # Shape n_head x d_head\n            v = self.r_r_bias * self.scale\n            # Shape d_model x n_head x d_head\n            w_r = self.r_kernel\n\n            # Shape max_rel_len x n_head x d_model\n            r_head = tf.einsum(\"td,dnh->tnh\", r, w_r)"}
{"text": "    def relative_positional_attention(self, position_embeds, q_head, context_len, cls_mask=None):\n        \"\"\"Relative attention score for the positional encodings\"\"\"\n        # q_head has shape batch_size x sea_len x n_head x d_head\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula (https://arxiv.org/abs/2006.03236)\n            # phi and pi have shape seq_len x d_model, psi and omega have shape context_len x d_model\n            phi, pi, psi, omega = position_embeds\n            # Shape n_head x d_head\n            u = self.r_r_bias * self.scale\n            # Shape d_model x n_head x d_head\n            w_r = self.r_kernel\n\n            # Shape batch_size x sea_len x n_head x d_model\n            q_r_attention = tf.einsum(\"binh,dnh->bind\", q_head + u, w_r)\n            q_r_attention_1 = q_r_attention * phi[:, None]\n            q_r_attention_2 = q_r_attention * pi[:, None]\n\n            # Shape batch_size x n_head x seq_len x context_len\n            positional_attn = tf.einsum(\"bind,jd->bnij\", q_r_attention_1, psi)\n        else:\n            # Notations from the paper, appending A.2.1, final formula (https://arxiv.org/abs/2006.03236)\n            # Grab the proper positional encoding, shape max_rel_len x d_model\n            if shape_list(q_head)[1] != context_len:\n                shift = 2\n                r = position_embeds[self.block_index][1]\n            else:\n                shift = 1\n                r = position_embeds[self.block_index][0]\n            # Shape n_head x d_head\n            v = self.r_r_bias * self.scale\n            # Shape d_model x n_head x d_head\n            w_r = self.r_kernel\n\n            # Shape max_rel_len x n_head x d_model\n            r_head = tf.einsum(\"td,dnh->tnh\", r, w_r)\n            # Shape batch_size x n_head x seq_len x max_rel_len\n            positional_attn = tf.einsum(\"binh,tnh->bnit\", q_head + v, r_head)"}
{"text": "    def relative_token_type_attention(self, token_type_mat, q_head, cls_mask=None):\n        \"\"\"Relative attention score for the token_type_ids\"\"\"\n        if token_type_mat is None:\n            return 0\n        batch_size, seq_len, context_len = shape_list(token_type_mat)\n        # q_head has shape batch_size x seq_len x n_head x d_head\n        # Shape n_head x d_head\n        r_s_bias = self.r_s_bias * self.scale\n\n        # Shape batch_size x n_head x seq_len x 2\n        token_type_bias = tf.einsum(\"bind,snd->bnis\", q_head + r_s_bias, self.seg_embed)"}
{"text": "    def call(self, query, key, value, attention_inputs, output_attentions=False, training=False):\n        # query has shape batch_size x seq_len x d_model\n        # key and value have shapes batch_size x context_len x d_model\n        position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs\n\n        batch_size, seq_len, _ = shape_list(query)\n        context_len = shape_list(key)[1]\n        n_head, d_head = self.n_head, self.d_head\n\n        # Shape batch_size x seq_len x n_head x d_head\n        q_head = tf.reshape(self.q_head(query), [batch_size, seq_len, n_head, d_head])\n        # Shapes batch_size x context_len x n_head x d_head\n        k_head = tf.reshape(self.k_head(key), [batch_size, context_len, n_head, d_head])\n        v_head = tf.reshape(self.v_head(value), [batch_size, context_len, n_head, d_head])\n\n        q_head = q_head * self.scale\n        # Shape n_head x d_head\n        r_w_bias = self.r_w_bias * self.scale\n        # Shapes batch_size x n_head x seq_len x context_len\n        content_score = tf.einsum(\"bind,bjnd->bnij\", q_head + r_w_bias, k_head)\n        positional_attn = self.relative_positional_attention(position_embeds, q_head, context_len, cls_mask)\n        token_type_attn = self.relative_token_type_attention(token_type_mat, q_head, cls_mask)\n\n        # merge attention scores\n        attn_score = content_score + positional_attn + token_type_attn\n\n        # perform masking\n        if attention_mask is not None:\n            attention_mask = tf.cast(attention_mask, dtype=attn_score.dtype)\n            attn_score = attn_score - (INF * (1 - attention_mask[:, None, None]))\n\n        # attention probability\n        attn_prob = tf.nn.softmax(attn_score, axis=-1)"}
{"text": "    def call(self, query, key, value, attention_inputs, output_attentions=False, training=False):\n        # query has shape batch_size x seq_len x d_model\n        # key and value have shapes batch_size x context_len x d_model\n        position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs\n\n        batch_size, seq_len, _ = shape_list(query)\n        context_len = shape_list(key)[1]\n        n_head, d_head = self.n_head, self.d_head\n\n        # Shape batch_size x seq_len x n_head x d_head\n        q_head = tf.reshape(self.q_head(query), [batch_size, seq_len, n_head, d_head])\n        # Shapes batch_size x context_len x n_head x d_head\n        k_head = tf.reshape(self.k_head(key), [batch_size, context_len, n_head, d_head])\n        v_head = tf.reshape(self.v_head(value), [batch_size, context_len, n_head, d_head])\n\n        q_head = q_head * self.scale\n        # Shape n_head x d_head\n        r_w_bias = self.r_w_bias * self.scale\n        # Shapes batch_size x n_head x seq_len x context_len\n        content_score = tf.einsum(\"bind,bjnd->bnij\", q_head + r_w_bias, k_head)\n        positional_attn = self.relative_positional_attention(position_embeds, q_head, context_len, cls_mask)\n        token_type_attn = self.relative_token_type_attention(token_type_mat, q_head, cls_mask)\n\n        # merge attention scores\n        attn_score = content_score + positional_attn + token_type_attn\n\n        # perform masking\n        if attention_mask is not None:\n            attention_mask = tf.cast(attention_mask, dtype=attn_score.dtype)\n            attn_score = attn_score - (INF * (1 - attention_mask[:, None, None]))\n\n        # attention probability\n        attn_prob = tf.nn.softmax(attn_score, axis=-1)\n        attn_prob = self.attention_dropout(attn_prob, training=training)\n\n        # attention output, shape batch_size x seq_len x n_head x d_head\n        attn_vec = tf.einsum(\"bnij,bjnd->bind\", attn_prob, v_head)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_1 = tf.keras.layers.Dense(config.d_inner, kernel_initializer=initializer, name=\"linear_1\")\n        self.activation_function = get_tf_activation(config.hidden_act)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.linear_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name=\"linear_2\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_1 = tf.keras.layers.Dense(config.d_inner, kernel_initializer=initializer, name=\"linear_1\")\n        self.activation_function = get_tf_activation(config.hidden_act)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.linear_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name=\"linear_2\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_1 = tf.keras.layers.Dense(config.d_inner, kernel_initializer=initializer, name=\"linear_1\")\n        self.activation_function = get_tf_activation(config.hidden_act)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.linear_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name=\"linear_2\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "def upsample(x, stride, target_len, separate_cls=True, truncate_seq=False):\n    \"\"\"\n    Upsample tensor `x` to match `target_len` by repeating the tokens `stride` time on the sequence length dimension.\n    \"\"\"\n    if stride == 1:\n        return x\n    if separate_cls:\n        cls = x[:, :1]\n        x = x[:, 1:]\n    output = tf.repeat(x, repeats=stride, axis=1)"}
{"text": "def upsample(x, stride, target_len, separate_cls=True, truncate_seq=False):\n    \"\"\"\n    Upsample tensor `x` to match `target_len` by repeating the tokens `stride` time on the sequence length dimension.\n    \"\"\"\n    if stride == 1:\n        return x\n    if separate_cls:\n        cls = x[:, :1]\n        x = x[:, 1:]\n    output = tf.repeat(x, repeats=stride, axis=1)\n    if separate_cls:\n        if truncate_seq:\n            output = tf.pad(output, [[0, 0], [0, stride - 1], [0, 0]])"}
{"text": "def upsample(x, stride, target_len, separate_cls=True, truncate_seq=False):\n    \"\"\"\n    Upsample tensor `x` to match `target_len` by repeating the tokens `stride` time on the sequence length dimension.\n    \"\"\"\n    if stride == 1:\n        return x\n    if separate_cls:\n        cls = x[:, :1]\n        x = x[:, 1:]\n    output = tf.repeat(x, repeats=stride, axis=1)\n    if separate_cls:\n        if truncate_seq:\n            output = tf.pad(output, [[0, 0], [0, stride - 1], [0, 0]])\n        output = output[:, : target_len - 1]\n        output = tf.concat([cls, output], axis=1)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.dense = tf.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name=\"dense\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.dense = tf.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name=\"dense\")\n        self.activation_function = get_tf_activation(config.hidden_act)\n        self.dense_prediction = tf.keras.layers.Dense(1, kernel_initializer=initializer, name=\"dense_prediction\")"}
{"text": "    def __init__(self, config, n_labels, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_hidden = tf.keras.layers.Dense(\n            config.d_model, kernel_initializer=initializer, name=\"linear_hidden\"\n        )"}
{"text": "    def __init__(self, config, n_labels, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_hidden = tf.keras.layers.Dense(\n            config.d_model, kernel_initializer=initializer, name=\"linear_hidden\"\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(input_shape, 0)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.funnel = TFFunnelMainLayer(config, name=\"funnel\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, nx, config, scale=False, is_cross_attention=False, **kwargs):\n        super().__init__(**kwargs)\n\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implementation]\n        assert n_state % config.n_head == 0\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.output_attentions = config.output_attentions\n\n        self.is_cross_attention = is_cross_attention\n\n        if self.is_cross_attention:\n            self.c_attn = TFConv1D(n_state * 2, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n            self.q_attn = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"q_attn\")\n        else:\n            self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n\n        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)"}
{"text": "    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)"}
{"text": "    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        if self.scale:\n            dk = tf.cast(shape_list(k)[-1], dtype=w.dtype)  # scale attention_scores\n            w = w / tf.math.sqrt(dk)\n\n        if not self.is_cross_attention:\n            # if only \"normal\" attention layer implements causal mask\n\n            # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n            _, _, nd, ns = shape_list(w)\n            b = self.causal_attention_mask(nd, ns, dtype=w.dtype)\n            b = tf.reshape(b, [1, 1, nd, ns])\n            w = w * b - 1e4 * (1 - b)\n\n        if attention_mask is not None:\n            # Apply the attention mask\n            attention_mask = tf.cast(attention_mask, dtype=w.dtype)\n            w = w + attention_mask\n\n        w = tf.nn.softmax(w, axis=-1)"}
{"text": "    def __init__(self, n_state, config, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n        self.act = get_tf_activation(config.activation_function)\n        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)"}
{"text": "    def __init__(self, config, scale=False, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")"}
{"text": "    def __init__(self, config, scale=False, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n        self.attn = TFAttention(nx, config, scale, name=\"attn\")\n        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")"}
{"text": "    def __init__(self, config, scale=False, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n        self.attn = TFAttention(nx, config, scale, name=\"attn\")\n        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")\n\n        if config.add_cross_attention:\n\n            self.crossattention = TFAttention(nx, config, scale, name=\"crossattention\", is_cross_attention=True)\n            self.ln_cross_attn = tf.keras.layers.LayerNormalization(\n                epsilon=config.layer_norm_epsilon, name=\"ln_cross_attn\"\n            )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(*inputs, **kwargs)\n\n        self.config = config\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.use_cache = config.use_cache\n        self.return_dict = config.use_return_dict\n\n        self.num_hidden_layers = config.n_layer\n        self.vocab_size = config.vocab_size\n        self.n_embd = config.n_embd\n        self.n_positions = config.n_positions\n        self.initializer_range = config.initializer_range\n\n        self.wte = TFSharedEmbeddings(\n            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n        )\n        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(*inputs, **kwargs)\n\n        self.config = config\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.use_cache = config.use_cache\n        self.return_dict = config.use_return_dict\n\n        self.num_hidden_layers = config.n_layer\n        self.vocab_size = config.vocab_size\n        self.n_embd = config.n_embd\n        self.n_positions = config.n_positions\n        self.initializer_range = config.initializer_range\n\n        self.wte = TFSharedEmbeddings(\n            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n        )\n        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n        self.h = [TFBlock(config, scale=True, name=f\"h_._{i}\") for i in range(config.n_layer)]\n        self.ln_f = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_f\")"}
{"text": "    def __init__(self, config: HubertConfig, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )"}
{"text": "    def __init__(self, config: HubertConfig, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.projection = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"projection\",\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.projection = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"projection\",\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.feat_proj_dropout)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.intermediate_dropout = tf.keras.layers.Dropout(config.activation_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.intermediate_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.intermediate_dense = tf.keras.layers.Dense(\n            units=config.intermediate_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"intermediate_dense\",\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.intermediate_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.intermediate_dense = tf.keras.layers.Dense(\n            units=config.intermediate_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"intermediate_dense\",\n        )\n        self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n\n        self.output_dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"output_dense\",\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.intermediate_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.intermediate_dense = tf.keras.layers.Dense(\n            units=config.intermediate_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"intermediate_dense\",\n        )\n        self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n\n        self.output_dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            bias_initializer=\"zeros\",\n            name=\"output_dense\",\n        )\n        self.output_dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.feed_forward = TFHubertFeedForward(config, name=\"feed_forward\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"final_layer_norm\"\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.feed_forward = TFHubertFeedForward(config, name=\"feed_forward\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"final_layer_norm\"\n        )"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.pos_conv_embed = TFHubertPositionalConvEmbedding(config, name=\"pos_conv_embed\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.pos_conv_embed = TFHubertPositionalConvEmbedding(config, name=\"pos_conv_embed\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.pos_conv_embed = TFHubertPositionalConvEmbedding(config, name=\"pos_conv_embed\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.pos_conv_embed = TFHubertPositionalConvEmbedding(config, name=\"pos_conv_embed\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: HubertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.hubert = TFHubertMainLayer(config, name=\"hubert\")\n        self.dropout = tf.keras.layers.Dropout(config.final_dropout)\n        self.lm_head = tf.keras.layers.Dense(config.vocab_size, name=\"lm_head\")"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.max_2d_position_embeddings = config.max_2d_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        bbox: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)"}
{"text": "layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)"}
{"text": "anspose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFLayoutLMModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = get_tf_activation(config.hidden_act)\n        else:\n            self.transform_act_fn = config.hidden_act\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "ype] = None,\n        bbox: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        encoder_hidden_states: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        encoder_attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf.Tensor]]:\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            bbox=bbox,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=input_shape, value=1)"}
{"text": "ray, tf.Tensor]] = None,\n        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        encoder_hidden_states: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        encoder_attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf.Tensor]]:\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            bbox=bbox,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=input_shape, value=1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: LayoutLMConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.layoutlm = TFLayoutLMMainLayer(config, add_pooling_layer=True, name=\"layoutlm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len)) * LARGE_NEGATIVE\n    mask_cond = tf.range(shape_list(mask)[-1])"}
{"text": "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_shape[:2]\n\n        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")"}
{"text": "\n        # attn_probs = (batch_size, seq_len, num_heads, window*2+1)\n        attn_scores = self._sliding_chunks_query_key_matmul(\n            query_vectors, key_vectors, self.one_sided_attn_window_size\n        )\n\n        # diagonal mask with zeros everywhere and -inf inplace of padding\n        diagonal_mask = self._sliding_chunks_query_key_matmul(\n            tf.ones(shape_list(attention_mask)),\n            attention_mask,\n            self.one_sided_attn_window_size,\n        )\n\n        # pad local attention probs\n        attn_scores += diagonal_mask\n\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_scores),\n                [batch_size, seq_len, self.num_heads, self.one_sided_attn_window_size * 2 + 1],\n                message=f\"attn_probs should be of size ({batch_size}, {seq_len}, {self.num_heads}, {self.one_sided_attn_window_size * 2 + 1}), but is of size {shape_list(attn_scores)}\",\n            )\n\n        # compute global attn indices required through out forward fn\n        (\n            max_num_global_attn_indices,\n            is_index_global_attn_nonzero,\n            is_local_index_global_attn_nonzero,\n            is_local_index_no_global_attn_nonzero,\n        ) = self._get_global_attn_indices(is_index_global_attn)\n\n        # this function is only relevant for global attention\n        attn_scores = tf.cond(\n            is_global_attn,\n            lambda: self._concat_with_global_key_attn_probs(\n                attn_scores=attn_scores,\n                query_vectors=query_vectors,\n                key_vectors=key_vectors,\n                max_num_global_attn_indices=max_num_global_attn_indices,\n                is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n                is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n                is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero,\n            ),\n            lambda: attn_scores,\n        )\n        attn_probs = tf.nn.softmax(attn_scores, axis=-1)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": "tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + tf.cast(\n                attention_mask, dtype=attn_weights.dtype\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: LEDConfig, layer_id: int, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDEncoderAttention(config, layer_id, name=\"self_attn\")\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: LEDConfig, layer_id: int, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDEncoderAttention(config, layer_id, name=\"self_attn\")\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: LEDConfig, layer_id: int, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDEncoderAttention(config, layer_id, name=\"self_attn\")\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: LEDConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDDecoderAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: LEDConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDDecoderAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFLEDDecoderAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")"}
{"text": "    def __init__(self, config: LEDConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDDecoderAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFLEDDecoderAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: LEDConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDDecoderAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFLEDDecoderAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def _pad_to_window_size(\n        self,\n        input_ids,\n        attention_mask,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)"}
{"text": "    def _pad_to_window_size(\n        self,\n        input_ids,\n        attention_mask,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)\n\n        if inputs_embeds is not None:\n\n            def pad_embeddings():\n                input_ids_padding = tf.fill((batch_size, padding_len), pad_token_id)\n                inputs_embeds_padding = self.embed_tokens(input_ids_padding)\n                return tf.concat([inputs_embeds, inputs_embeds_padding], axis=-2)\n\n            inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)\n\n        attention_mask = tf.pad(attention_mask, paddings, constant_values=False)"}
{"text": "    def __init__(self, config, input_embeddings, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.padding_idx = 1\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.padding_idx = 1\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        position_ids=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        past_key_values_length=0,\n        training=False,\n    ):\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "ap + 1,\n                message=\"attn_probs last dim has to be 2 * window_overlap + 1\",\n            )\n\n        chunks_count = seq_len // window_overlap - 1\n\n        # group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap\n        chunked_attn_probs = tf.reshape(\n            tf.transpose(attn_probs, (0, 2, 1, 3)),\n            (\n                batch_size * num_heads,\n                seq_len // window_overlap,\n                window_overlap,\n                2 * window_overlap + 1,\n            ),\n        )\n\n        # group batch_size and num_heads dimensions into one\n        value = tf.reshape(\n            tf.transpose(value, (0, 2, 1, 3)),\n            (batch_size * num_heads, seq_len, head_dim),\n        )\n\n        # pad seq_len with w at the beginning of the sequence and another window overlap at the end\n        paddings = tf.convert_to_tensor([[0, 0], [window_overlap, window_overlap], [0, 0]])\n        padded_value = tf.pad(value, paddings, constant_values=-1)\n\n        # chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap\n        frame_size = 3 * window_overlap * head_dim\n        frame_hop_size = (shape_list(padded_value)[1] * head_dim - frame_size) // chunks_count\n        chunked_value = tf.signal.frame(\n            tf.reshape(padded_value, (batch_size * num_heads, -1)),\n            frame_size,\n            frame_hop_size,\n        )\n        chunked_value = tf.reshape(\n            chunked_value,\n            (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim),\n        )\n\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(chunked_value),\n                [batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim],\n                message=\"Chunked value has the wrong shape\",\n            )\n\n        chunked_attn_probs = self._pad_and_diagonalize(chunked_attn_probs)\n        context = tf.einsum(\"bcwd,bcdh->bcwh\", chunked_attn_probs, chunked_value)"}
{"text": "    @staticmethod\n    def _pad_and_transpose_last_two_dims(hidden_states_padded, paddings):\n        \"\"\"pads rows and then flips rows and columns\"\"\"\n        hidden_states_padded = tf.pad(\n            hidden_states_padded, paddings\n        )"}
{"text": "    @staticmethod\n    def _get_global_attn_indices(is_index_global_attn):\n        \"\"\"compute global attn indices required throughout forward pass\"\"\"\n        # helper variable\n        num_global_attn_indices = tf.math.count_nonzero(is_index_global_attn, axis=1)\n        num_global_attn_indices = tf.cast(num_global_attn_indices, dtype=tf.constant(1).dtype)\n\n        # max number of global attn indices in batch\n        max_num_global_attn_indices = tf.reduce_max(num_global_attn_indices)\n\n        # indices of global attn\n        is_index_global_attn_nonzero = tf.where(is_index_global_attn)\n\n        # helper variable\n        is_local_index_global_attn = tf.range(max_num_global_attn_indices)"}
{"text": "shape_and_transpose(global_key_vectors, batch_size)\n        global_value_vectors = self.reshape_and_transpose(global_value_vectors, batch_size)\n\n        # compute attn scores\n        global_attn_scores = tf.matmul(global_query_vectors_only_global, global_key_vectors, transpose_b=True)\n\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(global_attn_scores),\n                [batch_size * self.num_heads, max_num_global_attn_indices, seq_len],\n                message=f\"global_attn_scores have the wrong size. Size should be {(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)}, but is {shape_list(global_attn_scores)}.\",\n            )\n\n        global_attn_scores = tf.reshape(\n            global_attn_scores,\n            (batch_size, self.num_heads, max_num_global_attn_indices, seq_len),\n        )\n        global_attn_scores_trans = tf.transpose(global_attn_scores, (0, 2, 1, 3))\n        mask_shape = (shape_list(is_local_index_no_global_attn_nonzero)[0],) + tuple(\n            shape_list(global_attn_scores_trans)[-2:]\n        )\n        global_attn_mask = tf.ones(mask_shape) * -10000.0\n        global_attn_mask = tf.cast(global_attn_mask, dtype=global_attn_scores_trans.dtype)\n\n        # scatter mask\n        global_attn_scores_trans = tf.tensor_scatter_nd_update(\n            global_attn_scores_trans,\n            is_local_index_no_global_attn_nonzero,\n            global_attn_mask,\n        )\n        global_attn_scores = tf.transpose(global_attn_scores_trans, (0, 2, 1, 3))\n\n        # mask global attn scores\n        attn_mask = tf.tile(is_index_masked[:, None, None, :], (1, shape_list(global_attn_scores)[1], 1, 1))\n        global_attn_scores = tf.where(attn_mask, -10000.0, global_attn_scores)\n        global_attn_scores = tf.reshape(\n            global_attn_scores,\n            (batch_size * self.num_heads, max_num_global_attn_indices, seq_len),\n        )\n\n        # compute global attn probs\n        global_attn_probs_float = tf.nn.softmax(global_attn_scores, axis=-1)"}
{"text": "    def _pad_to_window_size(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)"}
{"text": "    def _pad_to_window_size(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)\n\n        if position_ids is not None:\n            # pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings\n            position_ids = tf.pad(position_ids, paddings, constant_values=pad_token_id)"}
{"text": "    def _pad_to_window_size(\n        self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)\n\n        if position_ids is not None:\n            # pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings\n            position_ids = tf.pad(position_ids, paddings, constant_values=pad_token_id)\n\n        if inputs_embeds is not None:\n\n            def pad_embeddings():\n                input_ids_padding = tf.fill((batch_size, padding_len), self.pad_token_id)\n                inputs_embeds_padding = self.embeddings(input_ids_padding)\n                return tf.concat([inputs_embeds, inputs_embeds_padding], axis=-2)\n\n            inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)\n\n        attention_mask = tf.pad(attention_mask, paddings, constant_values=False)"}
{"text": "       input_ids,\n        attention_mask,\n        token_type_ids,\n        position_ids,\n        inputs_embeds,\n        pad_token_id,\n    ):\n        \"\"\"A helper function to pad tokens and mask to work with implementation of Longformer selfattention.\"\"\"\n        # padding\n        attention_window = (\n            self.attention_window if isinstance(self.attention_window, int) else max(self.attention_window)\n        )\n\n        assert attention_window % 2 == 0, f\"`attention_window` should be an even value. Given {attention_window}\"\n\n        input_shape = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)\n        batch_size, seq_len = input_shape[:2]\n        padding_len = (attention_window - seq_len % attention_window) % attention_window\n\n        if padding_len > 0:\n            logger.info(\n                f\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\n                f\"`config.attention_window`: {attention_window}\"\n            )\n\n        paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n        if input_ids is not None:\n            input_ids = tf.pad(input_ids, paddings, constant_values=pad_token_id)\n\n        if position_ids is not None:\n            # pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings\n            position_ids = tf.pad(position_ids, paddings, constant_values=pad_token_id)\n\n        if inputs_embeds is not None:\n\n            def pad_embeddings():\n                input_ids_padding = tf.fill((batch_size, padding_len), self.pad_token_id)\n                inputs_embeds_padding = self.embeddings(input_ids_padding)\n                return tf.concat([inputs_embeds, inputs_embeds_padding], axis=-2)\n\n            inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)\n\n        attention_mask = tf.pad(attention_mask, paddings, constant_values=False)  # no attention on the padding tokens\n        token_type_ids = tf.pad(token_type_ids, paddings, constant_values=0)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.longformer = TFLongformerMainLayer(config, add_pooling_layer=False, name=\"longformer\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"qa_outputs\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.out_proj = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.longformer = TFLongformerMainLayer(config, name=\"longformer\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.longformer = TFLongformerMainLayer(config=config, add_pooling_layer=False, name=\"longformer\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.longformer = TFLongformerMainLayer(config=config, add_pooling_layer=False, name=\"longformer\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(self, input_ids=None, token_type_ids=None, inputs_embeds=None, training=False):\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query\",\n        )\n        self.key = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"key\",\n        )\n        self.value = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"value\",\n        )"}
{"text": "    def call(self, hidden_states, context, attention_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(context)\n        mixed_value_layer = self.value(context)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFLxmertModel call() function)\n            attention_mask = tf.cast(attention_mask, dtype=attention_scores.dtype)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)"}
{"text": "    def call(self, hidden_states, context, attention_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(context)\n        mixed_value_layer = self.value(context)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFLxmertModel call() function)\n            attention_mask = tf.cast(attention_mask, dtype=attention_scores.dtype)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs, training=training)\n        context_layer = tf.matmul(attention_probs, value_layer)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.intermediate_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        # Object feature encoding\n        self.visn_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"visn_fc\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        # Object feature encoding\n        self.visn_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"visn_fc\",\n        )\n        self.visn_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"visn_layer_norm\"\n        )\n\n        # Box position encoding\n        self.box_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"box_fc\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        # Object feature encoding\n        self.visn_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"visn_fc\",\n        )\n        self.visn_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"visn_layer_norm\"\n        )\n\n        # Box position encoding\n        self.box_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"box_fc\",\n        )\n        self.box_layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"box_layer_norm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        # Object feature encoding\n        self.visn_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"visn_fc\",\n        )\n        self.visn_layer_norm = tf.keras.layers.LayerNormalization(\n            epsilon=config.layer_norm_eps, name=\"visn_layer_norm\"\n        )\n\n        # Box position encoding\n        self.box_fc = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"box_fc\",\n        )\n        self.box_layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"box_layer_norm\")\n\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: LxmertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, num_labels, **kwargs):\n        super().__init__(**kwargs)\n        hid_dim = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            hid_dim * 2,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"logit_fc_._0\",\n        )"}
{"text": "    def __init__(self, config, num_labels, **kwargs):\n        super().__init__(**kwargs)\n        hid_dim = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            hid_dim * 2,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"logit_fc_._0\",\n        )\n        self.activation = get_tf_activation(\"gelu\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"logit_fc_._2\")\n        self.dense_1 = tf.keras.layers.Dense(\n            num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"logit_fc_._3\",\n        )"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        visual_feats=None,\n        visual_pos=None,\n        attention_mask=None,\n        visual_attention_mask=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            visual_feats=visual_feats,\n            visual_pos=visual_pos,\n            attention_mask=attention_mask,\n            visual_attention_mask=visual_attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n        if inputs[\"visual_pos\"] is None or inputs[\"visual_feats\"] is None:\n            raise ValueError(\"visual_feats and visual_pos cannot be `None` in LXMERT's `call` method.\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(input_shape, 0)"}
{"text": "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = tf.ones((tgt_len, tgt_len))"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFMarianAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "    def __init__(self, config: MBartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n\n        self.embed_tokens = embed_tokens\n        self.embed_positions = TFMBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.layers = [TFMBartEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def __init__(self, config: MBartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n\n        self.embed_tokens = embed_tokens\n        self.embed_positions = TFMBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.layers = [TFMBartEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")"}
{"text": "    def __init__(self, config: MBartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.embed_tokens = embed_tokens\n        self.layerdrop = config.decoder_layerdrop\n        self.embed_positions = TFMBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n        self.layers = [TFMBartDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")"}
{"text": "    def __init__(self, config: MBartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.embed_tokens = embed_tokens\n        self.layerdrop = config.decoder_layerdrop\n        self.embed_positions = TFMBartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n        self.layers = [TFMBartDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n        self.layernorm_embedding = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.trigram_input = config.trigram_input\n        self.embedding_size = config.embedding_size\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.type_vocab_size = config.type_vocab_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.embedding_transformation = tf.keras.layers.Dense(config.hidden_size, name=\"embedding_transformation\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.trigram_input = config.trigram_input\n        self.embedding_size = config.embedding_size\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.type_vocab_size = config.type_vocab_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.embedding_transformation = tf.keras.layers.Dense(config.hidden_size, name=\"embedding_transformation\")\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = NORM2FN[config.normalization_type](\n            config.hidden_size, epsilon=config.layer_norm_eps, name=\"LayerNorm\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(self, input_ids=None, position_ids=None, token_type_ids=None, inputs_embeds=None, training=False):\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)\n\n        if self.trigram_input:\n            # From the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited\n            # Devices (https://arxiv.org/abs/2004.02984)\n            #\n            # The embedding table in BERT models accounts for a substantial proportion of model size. To compress\n            # the embedding layer, we reduce the embedding dimension to 128 in MobileBERT.\n            # Then, we apply a 1D convolution with kernel size 3 on the raw token embedding to produce a 512\n            # dimensional output.\n            inputs_embeds = tf.concat(\n                [\n                    tf.pad(inputs_embeds[:, 1:], ((0, 0), (0, 1), (0, 0)))"}
{"text": "    def call(self, input_ids=None, position_ids=None, token_type_ids=None, inputs_embeds=None, training=False):\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)\n\n        if self.trigram_input:\n            # From the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited\n            # Devices (https://arxiv.org/abs/2004.02984)\n            #\n            # The embedding table in BERT models accounts for a substantial proportion of model size. To compress\n            # the embedding layer, we reduce the embedding dimension to 128 in MobileBERT.\n            # Then, we apply a 1D convolution with kernel size 3 on the raw token embedding to produce a 512\n            # dimensional output.\n            inputs_embeds = tf.concat(\n                [\n                    tf.pad(inputs_embeds[:, 1:], ((0, 0), (0, 1), (0, 0))),\n                    inputs_embeds,\n                    tf.pad(inputs_embeds[:, :-1], ((0, 0), (1, 0), (0, 0)))"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.output_attentions = config.output_attentions\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.output_attentions = config.output_attentions\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.output_attentions = config.output_attentions\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)"}
{"text": "    def call(\n        self, query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions, training=False\n    ):\n        batch_size = shape_list(attention_mask)[0]\n        mixed_query_layer = self.query(query_tensor)\n        mixed_key_layer = self.key(key_tensor)\n        mixed_value_layer = self.value(value_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFMobileBertModel call() function)\n            attention_mask = tf.cast(attention_mask, dtype=attention_scores.dtype)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)"}
{"text": "    def call(\n        self, query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions, training=False\n    ):\n        batch_size = shape_list(attention_mask)[0]\n        mixed_query_layer = self.query(query_tensor)\n        mixed_key_layer = self.key(key_tensor)\n        mixed_value_layer = self.value(value_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFMobileBertModel call() function)\n            attention_mask = tf.cast(attention_mask, dtype=attention_scores.dtype)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = tf.matmul(attention_probs, value_layer)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.use_bottleneck = config.use_bottleneck\n        self.dense = tf.keras.layers.Dense(\n            config.true_hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.use_bottleneck = config.use_bottleneck\n        self.dense = tf.keras.layers.Dense(\n            config.true_hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = NORM2FN[config.normalization_type](\n            config.true_hidden_size, epsilon=config.layer_norm_eps, name=\"LayerNorm\"\n        )\n        if not self.use_bottleneck:\n            self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(config.intermediate_size, name=\"dense\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.hidden_size, name=\"dense\")\n        self.LayerNorm = NORM2FN[config.normalization_type](\n            config.hidden_size, epsilon=config.layer_norm_eps, name=\"LayerNorm\"\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.intra_bottleneck_size, name=\"dense\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(config.true_hidden_size, name=\"dense\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.do_activate = config.classifier_activation\n        if self.do_activate:\n            self.dense = tf.keras.layers.Dense(\n                config.hidden_size,\n                kernel_initializer=get_initializer(config.initializer_range),\n                activation=\"tanh\",\n                name=\"dense\",\n            )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(input_shape, 0)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.seq_relationship = tf.keras.layers.Dense(2, name=\"seq_relationship\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.mobilebert = TFMobileBertMainLayer(config, name=\"mobilebert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.mobilebert = TFMobileBertMainLayer(config, name=\"mobilebert\")\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.padding_idx = 1\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.padding_idx = 1\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.q = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"q\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.q = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"q\"\n        )\n        self.k = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"k\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.q = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"q\"\n        )\n        self.k = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"k\"\n        )\n        self.v = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"v\"\n        )\n        self.o = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"o\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.q = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"q\"\n        )\n        self.k = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"k\"\n        )\n        self.v = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"v\"\n        )\n        self.o = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"o\"\n        )\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, position_bias=None, training=False):\n        batch_size = shape_list(hidden_states)[0]\n\n        q = self.q(hidden_states)\n        k = self.k(hidden_states)\n        v = self.v(hidden_states)\n\n        q = self.transpose_for_scores(q, batch_size)\n        k = self.transpose_for_scores(k, batch_size)\n        v = self.transpose_for_scores(v, batch_size)\n\n        attention_scores = tf.matmul(q, k, transpose_b=True)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, position_bias=None, training=False):\n        batch_size = shape_list(hidden_states)[0]\n\n        q = self.q(hidden_states)\n        k = self.k(hidden_states)\n        v = self.v(hidden_states)\n\n        q = self.transpose_for_scores(q, batch_size)\n        k = self.transpose_for_scores(k, batch_size)\n        v = self.transpose_for_scores(v, batch_size)\n\n        attention_scores = tf.matmul(q, k, transpose_b=True)\n        dk = tf.cast(shape_list(k)[-1], attention_scores.dtype)\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        # Apply relative position embedding (precomputed in MPNetEncoder) if provided.\n        if position_bias is not None:\n            attention_scores += position_bias\n\n        if attention_mask is not None:\n            attention_scores = attention_scores + attention_mask\n\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, position_bias=None, training=False):\n        batch_size = shape_list(hidden_states)[0]\n\n        q = self.q(hidden_states)\n        k = self.k(hidden_states)\n        v = self.v(hidden_states)\n\n        q = self.transpose_for_scores(q, batch_size)\n        k = self.transpose_for_scores(k, batch_size)\n        v = self.transpose_for_scores(v, batch_size)\n\n        attention_scores = tf.matmul(q, k, transpose_b=True)\n        dk = tf.cast(shape_list(k)[-1], attention_scores.dtype)\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        # Apply relative position embedding (precomputed in MPNetEncoder) if provided.\n        if position_bias is not None:\n            attention_scores += position_bias\n\n        if attention_mask is not None:\n            attention_scores = attention_scores + attention_mask\n\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        attention_probs = self.dropout(attention_probs, training=training)\n\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        c = tf.matmul(attention_probs, v)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.attn = TFMPNetSelfAttention(config, name=\"attn\")\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: MPNetConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: MPNetConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: MPNetConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def compute_position_bias(self, x, position_ids=None):\n        \"\"\"Compute binned relative position bias\"\"\"\n        input_shape = shape_list(x)\n        qlen, klen = input_shape[1], input_shape[1]\n\n        if position_ids is not None:\n            context_position = position_ids[:, :, None]\n            memory_position = position_ids[:, None, :]\n        else:\n            context_position = tf.range(qlen)"}
{"text": "    @staticmethod\n    def _relative_position_bucket(relative_position, num_buckets=32, max_distance=128):\n        ret = 0\n        n = -relative_position\n\n        num_buckets //= 2\n        ret += tf.cast(tf.math.less(n, 0), dtype=relative_position.dtype) * num_buckets\n        n = tf.math.abs(n)\n\n        # now n is in the range [0, inf)\n        max_exact = num_buckets // 2\n        is_small = tf.math.less(n, max_exact)\n\n        val_if_large = max_exact + tf.cast(\n            tf.math.log(n / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact),\n            dtype=relative_position.dtype,\n        )\n\n        val_if_large = tf.math.minimum(val_if_large, num_buckets - 1)"}
{"text": "    @staticmethod\n    def _relative_position_bucket(relative_position, num_buckets=32, max_distance=128):\n        ret = 0\n        n = -relative_position\n\n        num_buckets //= 2\n        ret += tf.cast(tf.math.less(n, 0), dtype=relative_position.dtype) * num_buckets\n        n = tf.math.abs(n)\n\n        # now n is in the range [0, inf)\n        max_exact = num_buckets // 2\n        is_small = tf.math.less(n, max_exact)\n\n        val_if_large = max_exact + tf.cast(\n            tf.math.log(n / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact),\n            dtype=relative_position.dtype,\n        )\n\n        val_if_large = tf.math.minimum(val_if_large, num_buckets - 1)\n        ret += tf.where(is_small, n, val_if_large)"}
{"text": "    def __init__(self, config: MPNetConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)"}
{"text": "    def __init__(self, config, input_embeddings, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config, input_embeddings, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.mpnet = TFMPNetMainLayer(config, name=\"mpnet\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.mpnet = TFMPNetMainLayer(config, name=\"mpnet\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.mpnet = TFMPNetMainLayer(config, name=\"mpnet\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.mpnet = TFMPNetMainLayer(config, name=\"mpnet\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        self.out_proj = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\n        )"}
{"text": "    def __init__(self, nx, config, scale=False, **kwargs):\n        super().__init__(**kwargs)\n\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implementation]\n        assert (\n            n_state % config.n_head == 0\n        ), f\"Hidden dimension {n_state} not dividable by number of heads {config.n_head}\"\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.output_attentions = config.output_attentions\n\n        self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)"}
{"text": "    def __init__(self, n_state, config, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n        self.act = get_tf_activation(\"gelu\")\n        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)"}
{"text": "    def __init__(self, config, scale=False, **kwargs):\n        super().__init__(**kwargs)\n        nx = config.n_embd\n        self.attn = TFAttention(nx, config, scale, name=\"attn\")\n        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(*inputs, **kwargs)\n\n        self.config = config\n        self.output_hidden_states = config.output_hidden_states\n        self.output_attentions = config.output_attentions\n        self.return_dict = config.use_return_dict\n        self.num_hidden_layers = config.n_layer\n        self.vocab_size = config.vocab_size\n        self.n_embd = config.n_embd\n        self.n_positions = config.n_positions\n        self.initializer_range = config.initializer_range\n\n        self.tokens_embed = TFSharedEmbeddings(\n            config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name=\"tokens_embed\"\n        )\n        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.score = tf.keras.layers.Dense(\n            config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"score\",\n            use_bias=False,\n        )"}
{"text": "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_shape[:2]\n\n        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: PegasusConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n\n        self.embed_tokens = embed_tokens\n        self.embed_positions = TFPegasusSinusoidalPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.layers = [TFPegasusEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")"}
{"text": "    def get_nll(\n        self, seq_logits, doc_scores, target, reduce_loss=False, epsilon=0.0, exclude_bos_score=False, n_docs=None\n    ):\n        # shift tokens left\n        target = tf.concat([target[:, 1:], tf.fill([target.shape[0], 1], self.config.generator.pad_token_id)], axis=1)\n\n        # bos_token_id is None for T5\n        bos_token_id = self.config.bos_token_id or self.config.generator.bos_token_id\n        n_docs = n_docs if n_docs is not None else self.config.n_docs\n        equal_bos_token_id_all = tf.reduce_all(tf.equal(target[:, 0], bos_token_id))\n        use_bos = bos_token_id is not None and equal_bos_token_id_all\n\n        def _mask_pads(ll, smooth_obj):\n            pad_mask = tf.equal(target, self.config.generator.pad_token_id)\n            if tf.reduce_any(pad_mask):\n                ll = tf.where(pad_mask, 0.0, ll)\n                smooth_obj = tf.where(pad_mask, 0.0, smooth_obj)\n            return tf.squeeze(ll, axis=-1), tf.squeeze(smooth_obj, axis=-1)\n\n        # seq_logits.shape = (batch*n_docs, tgt_len , vocabs)\n        seq_logprobs = tf.nn.log_softmax(seq_logits, axis=-1)"}
{"text": "    def get_nll(\n        self, seq_logits, doc_scores, target, reduce_loss=False, epsilon=0.0, exclude_bos_score=False, n_docs=None\n    ):\n        # shift tokens left\n        target = tf.concat([target[:, 1:], tf.fill([target.shape[0], 1], self.config.generator.pad_token_id)], axis=1)\n\n        # bos_token_id is None for T5\n        bos_token_id = self.config.bos_token_id or self.config.generator.bos_token_id\n        n_docs = n_docs if n_docs is not None else self.config.n_docs\n        equal_bos_token_id_all = tf.reduce_all(tf.equal(target[:, 0], bos_token_id))\n        use_bos = bos_token_id is not None and equal_bos_token_id_all\n\n        def _mask_pads(ll, smooth_obj):\n            pad_mask = tf.equal(target, self.config.generator.pad_token_id)\n            if tf.reduce_any(pad_mask):\n                ll = tf.where(pad_mask, 0.0, ll)\n                smooth_obj = tf.where(pad_mask, 0.0, smooth_obj)\n            return tf.squeeze(ll, axis=-1), tf.squeeze(smooth_obj, axis=-1)\n\n        # seq_logits.shape = (batch*n_docs, tgt_len , vocabs)\n        seq_logprobs = tf.nn.log_softmax(seq_logits, axis=-1)\n        seq_logprobs = tf.reshape(\n            seq_logprobs, (seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.shape[-1])\n        )  # (batch_size, n_docs, tgt_len, vocabs)\n        doc_logprobs = tf.nn.log_softmax(doc_scores, axis=1)"}
{"text": "    def get_nll(\n        self, seq_logits, doc_scores, target, reduce_loss=False, epsilon=0.0, exclude_bos_score=False, n_docs=None\n    ):\n        # shift tokens left\n        target = tf.concat([target[:, 1:], tf.fill([target.shape[0], 1], self.config.generator.pad_token_id)], axis=1)\n\n        # bos_token_id is None for T5\n        bos_token_id = self.config.bos_token_id or self.config.generator.bos_token_id\n        n_docs = n_docs if n_docs is not None else self.config.n_docs\n        equal_bos_token_id_all = tf.reduce_all(tf.equal(target[:, 0], bos_token_id))\n        use_bos = bos_token_id is not None and equal_bos_token_id_all\n\n        def _mask_pads(ll, smooth_obj):\n            pad_mask = tf.equal(target, self.config.generator.pad_token_id)\n            if tf.reduce_any(pad_mask):\n                ll = tf.where(pad_mask, 0.0, ll)\n                smooth_obj = tf.where(pad_mask, 0.0, smooth_obj)\n            return tf.squeeze(ll, axis=-1), tf.squeeze(smooth_obj, axis=-1)\n\n        # seq_logits.shape = (batch*n_docs, tgt_len , vocabs)\n        seq_logprobs = tf.nn.log_softmax(seq_logits, axis=-1)\n        seq_logprobs = tf.reshape(\n            seq_logprobs, (seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.shape[-1])\n        )  # (batch_size, n_docs, tgt_len, vocabs)\n        doc_logprobs = tf.nn.log_softmax(doc_scores, axis=1)\n        doc_logprobs = tf.expand_dims(doc_logprobs, axis=-1)\n        doc_logprobs = tf.expand_dims(doc_logprobs, axis=-1)  # done twice to get 4-D\n\n        # RAG-sequence marginalization\n        first_token_scores = seq_logprobs[:, :, :1, :]\n        second_token_scores = seq_logprobs[:, :, 1:2, :]\n        remainder = seq_logprobs[:, :, 2:, :]\n        rag_logprobs = tf.concat([first_token_scores, second_token_scores + doc_logprobs, remainder], axis=2)"}
{"text": "    def marginalize(self, seq_logits, doc_scores, n_docs=None):\n        n_docs = n_docs if n_docs is not None else self.config.n_docs\n\n        # RAG-token marginalization\n        seq_logprobs = tf.nn.log_softmax(seq_logits, axis=-1)"}
{"text": "en_states = self.question_encoder(input_ids, attention_mask=attention_mask)[0]\n            out = self.retriever(\n                input_ids,\n                question_hidden_states.numpy().astype(np.float32),\n                prefix=self.generator.config.prefix,\n                n_docs=n_docs,\n                return_tensors=\"tf\",\n            )\n            context_input_ids, context_attention_mask, retrieved_doc_embeds = (\n                out[\"context_input_ids\"],\n                out[\"context_attention_mask\"],\n                out[\"retrieved_doc_embeds\"],\n            )\n\n            context_input_ids = tf.cast(context_input_ids, tf.int32)\n            context_attention_mask = tf.cast(context_attention_mask, tf.int32)\n            retrieved_doc_embeds = tf.cast(retrieved_doc_embeds, tf.float32)\n\n            # compute doc_scores\n            doc_scores = tf.matmul(\n                tf.expand_dims(question_hidden_states, axis=1), retrieved_doc_embeds, transpose_b=True\n            )\n            doc_scores = tf.squeeze(doc_scores, axis=1)\n\n        assert (\n            context_input_ids.shape[0] % n_docs\n        ) == 0, f\" The first dimension of `context_input_ids` should be a multiple of `n_docs`={n_docs}, but is {context_input_ids.shape[0]}.\"\n\n        batch_size = context_input_ids.shape[0] // n_docs\n\n        encoder = self.rag.generator.get_encoder()\n        encoder_outputs = encoder(\n            input_ids=context_input_ids,\n            attention_mask=context_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n        )\n\n        if return_dict_in_generate:\n            if output_attentions:\n                model_kwargs[\"encoder_attentions\"] = encoder_outputs.attentions\n            if output_hidden_states:\n                model_kwargs[\"encoder_hidden_states\"] = encoder_outputs.hidden_states\n\n        decoder_input_ids = tf.fill(\n            (batch_size * num_beams, 1),\n            tf.cast(decoder_start_token_id, tf.int32),\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.input_embedding_size = config.input_embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.type_vocab_size = config.type_vocab_size\n        self.input_embedding_size = config.input_embedding_size\n        self.max_position_embeddings = config.max_position_embeddings\n        self.initializer_range = config.initializer_range\n        self.embeddings_sum = tf.keras.layers.Add()\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        past_key_values_length=0,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n                f\"of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n\n        self.query = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )\n        self.value = tf.keras.layers.Dense(\n            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        encoder_hidden_states: tf.Tensor,\n        encoder_attention_mask: tf.Tensor,\n        past_key_value: Tuple[tf.Tensor],\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)"}
{"text": "layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(inputs=encoder_hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=encoder_hidden_states), batch_size)\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)"}
{"text": "ranspose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n            key_layer = tf.concatenate([past_key_value[0], key_layer], dim=2)\n            value_layer = tf.concatenate([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(inputs=hidden_states), batch_size)\n            value_layer = self.transpose_for_scores(self.value(inputs=hidden_states), batch_size)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFRemBertModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n\n        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"embedding_hidden_mapping_in\",\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, input_embeddings: tf.keras.layers.Layer, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.initializer_range = config.initializer_range\n        self.output_embedding_size = config.output_embedding_size\n        self.dense = tf.keras.layers.Dense(\n            config.output_embedding_size, kernel_initializer=get_initializer(self.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, input_embeddings: tf.keras.layers.Layer, **kwargs):\n        super().__init__(**kwargs)\n\n        self.vocab_size = config.vocab_size\n        self.initializer_range = config.initializer_range\n        self.output_embedding_size = config.output_embedding_size\n        self.dense = tf.keras.layers.Dense(\n            config.output_embedding_size, kernel_initializer=get_initializer(self.initializer_range), name=\"dense\"\n        )\n        if isinstance(config.hidden_act, str):\n            self.activation = get_tf_activation(config.hidden_act)\n        else:\n            self.activation = config.hidden_act\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: RemBertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.rembert = TFRemBertMainLayer(config, name=\"rembert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)"}
{"text": "    def __init__(self, config: RemBertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.rembert = TFRemBertMainLayer(config, name=\"rembert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.classifier_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"classifier\",\n        )"}
{"text": "    def __init__(self, config: RemBertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.rembert = TFRemBertMainLayer(config, add_pooling_layer=False, name=\"rembert\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        position_ids=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        past_key_values_length=0,\n        training=False,\n    ):\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        assert not (input_ids is None and inputs_embeds is None)\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def create_position_ids_from_input_ids(self, input_ids, past_key_values_length=0):\n        \"\"\"\n        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding\n        symbols are ignored. This is modified from fairseq's `utils.make_positions`.\n\n        Args:\n            input_ids: tf.Tensor\n        Returns: tf.Tensor\n        \"\"\"\n        mask = tf.cast(tf.math.not_equal(input_ids, self.padding_idx), dtype=input_ids.dtype)\n        incremental_indices = (tf.math.cumsum(mask, axis=1)"}
{"text": "    @staticmethod\n    def apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n        # https://kexue.fm/archives/8265\n        # sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n        # cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n        sin, cos = tf.split(sinusoidal_pos, num_or_size_splits=2, axis=-1)\n        # sin [\u03b80,\u03b81,\u03b82......\u03b8d/2-1]-> sin_pos [\u03b80,\u03b80,\u03b81,\u03b81,\u03b82,\u03b82......\u03b8d/2-1,\u03b8d/2-1]\n        # cos [\u03b80,\u03b81,\u03b82......\u03b8d/2-1]-> cos_pos [\u03b80,\u03b80,\u03b81,\u03b81,\u03b82,\u03b82......\u03b8d/2-1,\u03b8d/2-1]\n        sin_pos = tf.repeat(sin, 2, axis=-1)\n        cos_pos = tf.repeat(cos, 2, axis=-1)\n        # rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]\n        rotate_half_query_layer = tf.stack([-query_layer[..., 1::2], query_layer[..., ::2]], axis=-1)"}
{"text": "    @staticmethod\n    def apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n        # https://kexue.fm/archives/8265\n        # sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n        # cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n        sin, cos = tf.split(sinusoidal_pos, num_or_size_splits=2, axis=-1)\n        # sin [\u03b80,\u03b81,\u03b82......\u03b8d/2-1]-> sin_pos [\u03b80,\u03b80,\u03b81,\u03b81,\u03b82,\u03b82......\u03b8d/2-1,\u03b8d/2-1]\n        # cos [\u03b80,\u03b81,\u03b82......\u03b8d/2-1]-> cos_pos [\u03b80,\u03b80,\u03b81,\u03b81,\u03b82,\u03b82......\u03b8d/2-1,\u03b8d/2-1]\n        sin_pos = tf.repeat(sin, 2, axis=-1)\n        cos_pos = tf.repeat(cos, 2, axis=-1)\n        # rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]\n        rotate_half_query_layer = tf.stack([-query_layer[..., 1::2], query_layer[..., ::2]], axis=-1)\n        rotate_half_query_layer = tf.reshape(rotate_half_query_layer, shape_list(query_layer))\n        query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos\n        # rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]\n        rotate_half_key_layer = tf.stack([-key_layer[..., 1::2], key_layer[..., ::2]], axis=-1)"}
{"text": "    def call(\n        self,\n        input_ids: Optional[TFModelInputType] = None,\n        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=input_shape, value=1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        wi_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_model ** -0.5)\n        )\n        wo_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_ff ** -0.5)\n        )\n        self.wi = tf.keras.layers.Dense(\n            config.d_ff, use_bias=False, name=\"wi\", kernel_initializer=wi_initializer\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        wi_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_model ** -0.5)\n        )\n        wo_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_ff ** -0.5)\n        )\n        self.wi = tf.keras.layers.Dense(\n            config.d_ff, use_bias=False, name=\"wi\", kernel_initializer=wi_initializer\n        )  # Update init weights as in flax\n        self.wo = tf.keras.layers.Dense(\n            config.d_model, use_bias=False, name=\"wo\", kernel_initializer=wo_initializer\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        wi_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_model ** -0.5)\n        )\n        wo_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (config.d_ff ** -0.5)\n        )\n        self.wi = tf.keras.layers.Dense(\n            config.d_ff, use_bias=False, name=\"wi\", kernel_initializer=wi_initializer\n        )  # Update init weights as in flax\n        self.wo = tf.keras.layers.Dense(\n            config.d_model, use_bias=False, name=\"wo\", kernel_initializer=wo_initializer\n        )  # Update init weights as in flax\n        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.feed_forward_proj == \"relu\":\n            self.DenseReluDense = TFT5DenseReluDense(config, name=\"DenseReluDense\")\n        elif config.feed_forward_proj == \"gated-gelu\":\n            self.DenseReluDense = TFT5GatedGeluDense(config, name=\"DenseReluDense\")\n        else:\n            raise ValueError(\n                f\"{self.config.feed_forward_proj} is not supported. Choose between `relu` and `gated-gelu`\"\n            )\n        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)"}
{"text": "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n        super().__init__(**kwargs)\n        self.layer_id = next(TFT5Attention.NEW_ID)\n        self.is_decoder = config.is_decoder\n        self.use_cache = config.use_cache\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.output_attentions = config.output_attentions\n\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv\n        self.n_heads = config.num_heads\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        q_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        )\n        k_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        v_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        o_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        self.relative_attention_bias_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n\n        self.q = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"q\", kernel_initializer=q_initializer\n        )"}
{"text": "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n        super().__init__(**kwargs)\n        self.layer_id = next(TFT5Attention.NEW_ID)\n        self.is_decoder = config.is_decoder\n        self.use_cache = config.use_cache\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.output_attentions = config.output_attentions\n\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv\n        self.n_heads = config.num_heads\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        q_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        )\n        k_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        v_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        o_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        self.relative_attention_bias_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n\n        self.q = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"q\", kernel_initializer=q_initializer\n        )  # Update init weights as in flax\n        self.k = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"k\", kernel_initializer=k_initializer\n        )"}
{"text": "id = next(TFT5Attention.NEW_ID)\n        self.is_decoder = config.is_decoder\n        self.use_cache = config.use_cache\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.output_attentions = config.output_attentions\n\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv\n        self.n_heads = config.num_heads\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        q_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        )\n        k_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        v_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        o_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n        self.relative_attention_bias_initializer = tf.keras.initializers.RandomNormal(\n            mean=0, stddev=config.initializer_factor * (self.inner_dim ** -0.5)\n        )\n\n        self.q = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"q\", kernel_initializer=q_initializer\n        )  # Update init weights as in flax\n        self.k = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"k\", kernel_initializer=k_initializer\n        )  # Update init weights as in flax\n        self.v = tf.keras.layers.Dense(\n            self.inner_dim, use_bias=False, name=\"v\", kernel_initializer=v_initializer\n        )  # Update init weights as in flax\n        self.o = tf.keras.layers.Dense(\n            self.d_model, use_bias=False, name=\"o\", kernel_initializer=o_initializer\n        )"}
{"text": "    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n        \"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        #        n = -relative_position\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (\n                tf.cast(tf.math.greater(relative_position, 0), dtype=relative_position.dtype) * num_buckets\n            )\n            relative_position = tf.math.abs(relative_position)\n        else:\n            relative_position = -tf.math.minimum(relative_position, 0)"}
{"text": "def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n        \"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        #        n = -relative_position\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (\n                tf.cast(tf.math.greater(relative_position, 0), dtype=relative_position.dtype) * num_buckets\n            )\n            relative_position = tf.math.abs(relative_position)\n        else:\n            relative_position = -tf.math.minimum(relative_position, 0)\n        # now n is in the range [0, inf)\n        max_exact = num_buckets // 2\n        is_small = tf.math.less(relative_position, max_exact)\n        relative_position_if_large = max_exact + tf.cast(\n            tf.math.log(relative_position / max_exact)\n            "}
{"text": "ive position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        #        n = -relative_position\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (\n                tf.cast(tf.math.greater(relative_position, 0), dtype=relative_position.dtype) * num_buckets\n            )\n            relative_position = tf.math.abs(relative_position)\n        else:\n            relative_position = -tf.math.minimum(relative_position, 0)\n        # now n is in the range [0, inf)\n        max_exact = num_buckets // 2\n        is_small = tf.math.less(relative_position, max_exact)\n        relative_position_if_large = max_exact + tf.cast(\n            tf.math.log(relative_position / max_exact)\n            / math.log(max_distance / max_exact)\n            * (num_buckets - max_exact),\n            dtype=relative_position.dtype,\n        )\n        relative_position_if_large = tf.math.minimum(relative_position_if_large, num_buckets - 1)\n        relative_buckets += tf.where(is_small, relative_position, relative_position_if_large)"}
{"text": "    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = tf.range(query_length)"}
{"text": "    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = tf.range(query_length)[:, None]\n        memory_position = tf.range(key_length)"}
{"text": "        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n            \"\"\"projects hidden states correctly to key/query states\"\"\"\n            if key_value_states is None:\n                # self-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(hidden_states))\n            elif past_key_value is None:\n                # cross-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(key_value_states))\n\n            if past_key_value is not None:\n                if key_value_states is None:\n                    # self-attn\n                    # (batch_size, n_heads, key_length, dim_per_head)\n                    hidden_states = tf.concat([past_key_value, hidden_states], axis=2)"}
{"text": "(proj_layer(key_value_states))\n\n            if past_key_value is not None:\n                if key_value_states is None:\n                    # self-attn\n                    # (batch_size, n_heads, key_length, dim_per_head)\n                    hidden_states = tf.concat([past_key_value, hidden_states], axis=2)\n                else:\n                    # cross-attn\n                    hidden_states = past_key_value\n            return hidden_states\n\n        # get query\n        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, query_length, dim_per_head)\n\n        # get key/value\n        key_states = project(\n            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n        )\n        value_states = project(\n            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n        )\n\n        # to cope with keras serialization\n        if self.is_decoder and use_cache:\n            present_key_value_state = (key_states, value_states)\n        else:\n            present_key_value_state = None\n\n        scores = tf.einsum(\n            \"bnqd,bnkd->bnqk\", query_states, key_states\n        )  # (batch_size, n_heads, query_length, key_length)\n\n        if position_bias is None:\n            if not self.has_relative_attention_bias:\n                position_bias = tf.zeros((1, self.n_heads, real_seq_length, key_length))\n            else:\n                position_bias = self.compute_bias(real_seq_length, key_length)\n\n            # if key and values are already calculated\n            # we want only the last query position bias\n            if past_key_value is not None:\n                position_bias = position_bias[:, :, -seq_length:, :]\n\n            if mask is not None:\n                position_bias = tf.cast(position_bias, dtype=mask.dtype)\n                position_bias = position_bias + mask  # (batch_size, n_heads, query_length, key_length)\n\n        scores += position_bias\n        weights = tf.nn.softmax(scores, axis=-1)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.EncDecAttention = TFT5Attention(\n            config,\n            has_relative_attention_bias=False,\n            name=\"EncDecAttention\",\n        )\n        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)"}
{"text": "    def _shift_right(self, input_ids):\n        decoder_start_token_id = self.config.decoder_start_token_id\n        pad_token_id = self.config.pad_token_id\n\n        assert (\n            decoder_start_token_id is not None\n        ), \"self.model.config.decoder_start_token_id has to be defined. In TF T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n\n        start_tokens = tf.fill((shape_list(input_ids)[0], 1), decoder_start_token_id)"}
{"text": "    def _shift_right(self, input_ids):\n        decoder_start_token_id = self.config.decoder_start_token_id\n        pad_token_id = self.config.pad_token_id\n\n        assert (\n            decoder_start_token_id is not None\n        ), \"self.model.config.decoder_start_token_id has to be defined. In TF T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n\n        start_tokens = tf.fill((shape_list(input_ids)[0], 1), decoder_start_token_id)\n        start_tokens = tf.cast(start_tokens, input_ids.dtype)  # Ensure compatible dtypes for concatenation\n        shifted_input_ids = tf.concat([start_tokens, input_ids[:, :-1]], -1)"}
{"text": "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.layer_1 = tf.keras.layers.Dense(\n            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n        )"}
{"text": "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.layer_1 = tf.keras.layers.Dense(\n            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n        )\n        self.drop_1 = tf.keras.layers.Dropout(dropout)"}
{"text": "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.layer_1 = tf.keras.layers.Dense(\n            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n        )\n        self.drop_1 = tf.keras.layers.Dropout(dropout)\n        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")"}
{"text": "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.layer_1 = tf.keras.layers.Dense(\n            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n        )\n        self.drop_1 = tf.keras.layers.Dropout(dropout)\n        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n        self.drop_2 = tf.keras.layers.Dropout(dropout)"}
{"text": "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.layer_1 = tf.keras.layers.Dense(\n            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n        )\n        self.drop_1 = tf.keras.layers.Dropout(dropout)\n        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n        self.drop_2 = tf.keras.layers.Dropout(dropout)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")"}
{"text": "    def __init__(\n        self,\n        n_head,\n        d_model,\n        d_head,\n        dropout,\n        dropatt=0.0,\n        pre_lnorm=False,\n        r_r_bias=None,\n        r_w_bias=None,\n        layer_norm_epsilon=1e-5,\n        init_std=0.02,\n        output_attentions=False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n        self.output_attentions = output_attentions\n\n        self.qkv_net = tf.keras.layers.Dense(\n            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n        )"}
{"text": "    def __init__(\n        self,\n        n_head,\n        d_model,\n        d_head,\n        dropout,\n        dropatt=0.0,\n        pre_lnorm=False,\n        r_r_bias=None,\n        r_w_bias=None,\n        layer_norm_epsilon=1e-5,\n        init_std=0.02,\n        output_attentions=False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n        self.output_attentions = output_attentions\n\n        self.qkv_net = tf.keras.layers.Dense(\n            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n        )\n\n        self.drop = tf.keras.layers.Dropout(dropout)"}
{"text": "    def __init__(\n        self,\n        n_head,\n        d_model,\n        d_head,\n        dropout,\n        dropatt=0.0,\n        pre_lnorm=False,\n        r_r_bias=None,\n        r_w_bias=None,\n        layer_norm_epsilon=1e-5,\n        init_std=0.02,\n        output_attentions=False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n        self.output_attentions = output_attentions\n\n        self.qkv_net = tf.keras.layers.Dense(\n            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n        )\n\n        self.drop = tf.keras.layers.Dropout(dropout)\n        self.dropatt = tf.keras.layers.Dropout(dropatt)"}
{"text": "    def __init__(\n        self,\n        n_head,\n        d_model,\n        d_head,\n        dropout,\n        dropatt=0.0,\n        pre_lnorm=False,\n        r_r_bias=None,\n        r_w_bias=None,\n        layer_norm_epsilon=1e-5,\n        init_std=0.02,\n        output_attentions=False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n        self.output_attentions = output_attentions\n\n        self.qkv_net = tf.keras.layers.Dense(\n            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n        )\n\n        self.drop = tf.keras.layers.Dropout(dropout)\n        self.dropatt = tf.keras.layers.Dropout(dropatt)\n        self.o_net = tf.keras.layers.Dense(\n            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n        )\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")"}
{"text": "    def __init__(\n        self,\n        n_head,\n        d_model,\n        d_head,\n        dropout,\n        dropatt=0.0,\n        pre_lnorm=False,\n        r_r_bias=None,\n        r_w_bias=None,\n        layer_norm_epsilon=1e-5,\n        init_std=0.02,\n        output_attentions=False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n        self.output_attentions = output_attentions\n\n        self.qkv_net = tf.keras.layers.Dense(\n            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n        )\n\n        self.drop = tf.keras.layers.Dropout(dropout)\n        self.dropatt = tf.keras.layers.Dropout(dropatt)\n        self.o_net = tf.keras.layers.Dense(\n            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n        )\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n        else:\n            self.r_r_bias = None\n            self.r_w_bias = None\n\n        self.r_net = tf.keras.layers.Dense(\n            self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"r_net\"\n        )"}
{"text": "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n\n        if mems is not None:\n            mems = tf.cast(mems, dtype=w.dtype)\n            cat = tf.concat([mems, w], 0)"}
{"text": "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n\n        if mems is not None:\n            mems = tf.cast(mems, dtype=w.dtype)\n            cat = tf.concat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)"}
{"text": "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n\n        if mems is not None:\n            mems = tf.cast(mems, dtype=w.dtype)\n            cat = tf.concat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n\n        klen = shape_list(w_head_k)[0]\n\n        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n\n        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n\n        # compute attention score\n        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)"}
{"text": "ist(w)[1]\n\n        if mems is not None:\n            mems = tf.cast(mems, dtype=w.dtype)\n            cat = tf.concat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n\n        klen = shape_list(w_head_k)[0]\n\n        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n\n        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n\n        # compute attention score\n        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score = attn_score * self.scale\n\n        # compute attention probability\n        if attn_mask is not None:\n            attn_mask_t = attn_mask[:, :, None, None]\n            attn_mask_t = tf.cast(attn_mask_t, dtype=attn_score.dtype)\n            attn_score = attn_score * (1.0 - attn_mask_t) - 1e30 * attn_mask_t\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = tf.nn.softmax(attn_score, axis=1)"}
{"text": "            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n\n        klen = shape_list(w_head_k)[0]\n\n        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n\n        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n\n        # compute attention score\n        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score = attn_score * self.scale\n\n        # compute attention probability\n        if attn_mask is not None:\n            attn_mask_t = attn_mask[:, :, None, None]\n            attn_mask_t = tf.cast(attn_mask_t, dtype=attn_score.dtype)\n            attn_score = attn_score * (1.0 - attn_mask_t) - 1e30 * attn_mask_t\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = tf.nn.softmax(attn_score, axis=1)\n        attn_prob = self.dropatt(attn_prob, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_prob = attn_prob * head_mask\n\n        # compute attention vector\n        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)"}
{"text": "    def init_mems(self, bsz):\n        if self.mem_len > 0:\n            mems = []\n            for i in range(self.n_layer):\n                empty = tf.zeros([self.mem_len, bsz, self.d_model])"}
{"text": "    def _update_mems(self, hids, mems, mlen, qlen):\n        # does not deal with None\n        if mems is None:\n            return None\n\n        # mems is not None\n        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        new_mems = []\n        end_idx = mlen + tf.math.maximum(0, qlen)\n        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))\n        for i in range(len(hids)):\n            mems[i] = tf.cast(mems[i], dtype=hids[i].dtype)\n            cat = tf.concat([mems[i], hids[i]], axis=0)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.score = tf.keras.layers.Dense(\n            config.num_labels,\n            kernel_initializer=get_initializer(config.init_range),\n            name=\"score\",\n            use_bias=False,\n        )"}
{"text": "    def interpolate_pos_encoding(self, embeddings, height, width) -> tf.Tensor:\n        \"\"\"\n        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n        resolution images.\n\n        Source:\n        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n        \"\"\"\n\n        batch_size, seq_len, dim = shape_list(embeddings)\n        npatch = seq_len - 1\n\n        _, N, _ = shape_list(self.position_embeddings)\n        N -= 1\n\n        if npatch == N and height == width:\n            return self.position_embeddings\n        class_pos_embed = self.position_embeddings[:, :1]\n        patch_pos_embed = self.position_embeddings[:, 1:]\n        h0 = height // self.config.patch_size\n        w0 = width // self.config.patch_size\n        patch_pos_embed = tf.image.resize(\n            images=tf.reshape(patch_pos_embed, shape=(1, int(math.sqrt(N)), int(math.sqrt(N)), dim)),\n            size=(h0, w0),\n            method=\"bicubic\",\n        )\n\n        shape = shape_list(patch_pos_embed)\n        assert h0 == shape[-3] and w0 == shape[-2]\n        patch_pos_embed = tf.reshape(tensor=patch_pos_embed, shape=(1, -1, dim))\n        return tf.concat(values=(class_pos_embed, patch_pos_embed), axis=1)"}
{"text": "    def call(\n        self, pixel_values: tf.Tensor, interpolate_pos_encoding: bool = False, training: bool = False\n    ) -> tf.Tensor:\n        batch_size, num_channels, height, width = shape_list(pixel_values)\n        embeddings = self.patch_embeddings(\n            pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, training=training\n        )\n\n        # add the [CLS] token to the embedded patch tokens\n        cls_tokens = tf.repeat(self.cls_token, repeats=batch_size, axis=0)\n        embeddings = tf.concat((cls_tokens, embeddings), axis=1)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        head_mask: tf.Tensor,\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(inputs=hidden_states)\n        mixed_key_layer = self.key(inputs=hidden_states)\n        mixed_value_layer = self.value(inputs=hidden_states)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "    def __init__(self, config: ViTConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: Wav2Vec2Config, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )"}
{"text": "    def __init__(self, config: Wav2Vec2Config, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )"}
{"text": "    def __init__(self, config: Wav2Vec2Config, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )\n        self.layer_norm = tf.keras.layers.LayerNormalization(name=\"layer_norm\", epsilon=config.layer_norm_eps)"}
{"text": "    def __init__(self, config: Wav2Vec2Config, layer_id: int = 0, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n        self.out_conv_dim = config.conv_dim[layer_id]\n\n        self.conv = tf.keras.layers.Conv1D(\n            filters=self.out_conv_dim,\n            kernel_size=config.conv_kernel[layer_id],\n            strides=config.conv_stride[layer_id],\n            use_bias=config.conv_bias,\n            name=\"conv\",\n        )"}
{"text": "    def __init__(self, config: Wav2Vec2Config, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.pos_conv_embed = TFWav2Vec2PositionalConvEmbedding(config, name=\"pos_conv_embed\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)"}
{"text": "    def __init__(self, config: Wav2Vec2Config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.wav2vec2 = TFWav2Vec2MainLayer(config, name=\"wav2vec2\")\n        self.dropout = tf.keras.layers.Dropout(config.final_dropout)"}
{"text": "dicted_ids[0])\n\n            >>> # compute loss\n            >>> target_transcription = \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n\n            >>> # wrap processor as target processor to encode labels\n            >>> with processor.as_target_processor():\n            >>>     labels = processor(transcription, return_tensors=\"tf\").input_ids\n\n            >>> loss = model(input_values, labels=labels).loss\n        \"\"\"\n        inputs = input_values_processing(\n            func=self.call,\n            config=self.config,\n            input_values=input_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n        )\n\n        outputs = self.wav2vec2(\n            input_values=inputs[\"input_values\"],\n            attention_mask=inputs[\"attention_mask\"],\n            token_type_ids=inputs[\"token_type_ids\"],\n            position_ids=inputs[\"position_ids\"],\n            head_mask=inputs[\"head_mask\"],\n            inputs_embeds=inputs[\"inputs_embeds\"],\n            output_attentions=inputs[\"output_attentions\"],\n            output_hidden_states=inputs[\"output_hidden_states\"],\n            return_dict=inputs[\"return_dict\"],\n            training=inputs[\"training\"],\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n\n        logits = self.lm_head(hidden_states)\n\n        if labels is not None:\n\n            if tf.reduce_max(labels) >= self.config.vocab_size:\n                raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n\n            attention_mask = (\n                inputs[\"attention_mask\"]\n                if inputs[\"attention_mask\"] is not None\n                else tf.ones_like(inputs, dtype=tf.float32)\n            "}
{"text": "    def call(self, input, mask, kv, cache, head_mask, output_attentions, training=False):\n        \"\"\"\n        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n        \"\"\"\n        # Input is (bs, qlen, dim)\n        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n        bs, qlen, dim = shape_list(input)\n\n        if kv is None:\n            klen = qlen if cache is None else cache[\"slen\"] + qlen\n        else:\n            klen = shape_list(kv)[1]\n\n        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n        dim_per_head = self.dim // self.n_heads\n        mask_reshape = (bs, 1, qlen, klen) if len(shape_list(mask)) == 3 else (bs, 1, 1, klen)\n\n        def shape(x):\n            \"\"\"projection\"\"\"\n            return tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"compute context\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n\n        if kv is None:\n            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n        elif cache is None or self.layer_id not in cache:\n            k = v = kv\n            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n\n        if cache is not None:\n            if self.layer_id in cache:\n                if kv is None:\n                    k_, v_ = cache[self.layer_id]\n                    k = tf.concat([k_, k], axis=2)"}
{"text": "    def call(self, input, mask, kv, cache, head_mask, output_attentions, training=False):\n        \"\"\"\n        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n        \"\"\"\n        # Input is (bs, qlen, dim)\n        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n        bs, qlen, dim = shape_list(input)\n\n        if kv is None:\n            klen = qlen if cache is None else cache[\"slen\"] + qlen\n        else:\n            klen = shape_list(kv)[1]\n\n        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n        dim_per_head = self.dim // self.n_heads\n        mask_reshape = (bs, 1, qlen, klen) if len(shape_list(mask)) == 3 else (bs, 1, 1, klen)\n\n        def shape(x):\n            \"\"\"projection\"\"\"\n            return tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"compute context\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n\n        if kv is None:\n            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n        elif cache is None or self.layer_id not in cache:\n            k = v = kv\n            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n\n        if cache is not None:\n            if self.layer_id in cache:\n                if kv is None:\n                    k_, v_ = cache[self.layer_id]\n                    k = tf.concat([k_, k], axis=2)  # (bs, n_heads, klen, dim_per_head)\n                    v = tf.concat([v_, v], axis=2)"}
{"text": "ensions do not match: {dim} input vs {self.dim} configured'\n        dim_per_head = self.dim // self.n_heads\n        mask_reshape = (bs, 1, qlen, klen) if len(shape_list(mask)) == 3 else (bs, 1, 1, klen)\n\n        def shape(x):\n            \"\"\"projection\"\"\"\n            return tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"compute context\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n\n        if kv is None:\n            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n        elif cache is None or self.layer_id not in cache:\n            k = v = kv\n            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n\n        if cache is not None:\n            if self.layer_id in cache:\n                if kv is None:\n                    k_, v_ = cache[self.layer_id]\n                    k = tf.concat([k_, k], axis=2)  # (bs, n_heads, klen, dim_per_head)\n                    v = tf.concat([v_, v], axis=2)  # (bs, n_heads, klen, dim_per_head)\n                else:\n                    k, v = cache[self.layer_id]\n\n            cache[self.layer_id] = (k, v)\n\n        f_dim_per_head = tf.cast(dim_per_head, dtype=q.dtype)\n        q = tf.multiply(q, tf.math.rsqrt(f_dim_per_head))  # (bs, n_heads, qlen, dim_per_head)\n        k = tf.cast(k, dtype=q.dtype)\n        scores = tf.matmul(q, k, transpose_b=True)  # (bs, n_heads, qlen, klen)\n        mask = tf.reshape(mask, mask_reshape)  # (bs, n_heads, qlen, klen)\n        # scores.masked_fill_(mask, -float('inf'))                            # (bs, n_heads, qlen, klen)\n        mask = tf.cast(mask, dtype=scores.dtype)\n        scores = scores - 1e30 * (1.0 - mask)\n        weights = tf.nn.softmax(scores, axis=-1)"}
{"text": "urn tf.transpose(tf.reshape(x, (bs, -1, self.n_heads, dim_per_head)), perm=(0, 2, 1, 3))\n\n        def unshape(x):\n            \"\"\"compute context\"\"\"\n            return tf.reshape(tf.transpose(x, perm=(0, 2, 1, 3)), (bs, -1, self.n_heads * dim_per_head))\n\n        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n\n        if kv is None:\n            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n        elif cache is None or self.layer_id not in cache:\n            k = v = kv\n            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n\n        if cache is not None:\n            if self.layer_id in cache:\n                if kv is None:\n                    k_, v_ = cache[self.layer_id]\n                    k = tf.concat([k_, k], axis=2)  # (bs, n_heads, klen, dim_per_head)\n                    v = tf.concat([v_, v], axis=2)  # (bs, n_heads, klen, dim_per_head)\n                else:\n                    k, v = cache[self.layer_id]\n\n            cache[self.layer_id] = (k, v)\n\n        f_dim_per_head = tf.cast(dim_per_head, dtype=q.dtype)\n        q = tf.multiply(q, tf.math.rsqrt(f_dim_per_head))  # (bs, n_heads, qlen, dim_per_head)\n        k = tf.cast(k, dtype=q.dtype)\n        scores = tf.matmul(q, k, transpose_b=True)  # (bs, n_heads, qlen, klen)\n        mask = tf.reshape(mask, mask_reshape)  # (bs, n_heads, qlen, klen)\n        # scores.masked_fill_(mask, -float('inf'))                            # (bs, n_heads, qlen, klen)\n        mask = tf.cast(mask, dtype=scores.dtype)\n        scores = scores - 1e30 * (1.0 - mask)\n        weights = tf.nn.softmax(scores, axis=-1)  # (bs, n_heads, qlen, klen)\n        weights = self.dropout(weights, training=training)  # (bs, n_heads, qlen, klen)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            weights = weights * head_mask\n\n        context = tf.matmul(weights, v)"}
{"text": "    def __init__(self, in_dim, dim_hidden, out_dim, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.lin1 = tf.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name=\"lin1\")"}
{"text": "    def __init__(self, in_dim, dim_hidden, out_dim, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.lin1 = tf.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name=\"lin1\")\n        self.lin2 = tf.keras.layers.Dense(out_dim, kernel_initializer=get_initializer(config.init_std), name=\"lin2\")"}
{"text": "    def prepare_inputs_for_generation(self, inputs, **kwargs):\n        mask_token_id = self.config.mask_token_id\n        lang_id = self.config.lang_id\n\n        effective_batch_size = inputs.shape[0]\n        mask_token = tf.fill((effective_batch_size, 1), 1) * mask_token_id\n        inputs = tf.concat([inputs, mask_token], axis=1)"}
{"text": "    def prepare_inputs_for_generation(self, inputs, **kwargs):\n        mask_token_id = self.config.mask_token_id\n        lang_id = self.config.lang_id\n\n        effective_batch_size = inputs.shape[0]\n        mask_token = tf.fill((effective_batch_size, 1), 1) * mask_token_id\n        inputs = tf.concat([inputs, mask_token], axis=1)\n\n        if lang_id is not None:\n            langs = tf.ones_like(inputs)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.transformer = TFXLMMainLayer(config, name=\"transformer\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.init_std), name=\"qa_outputs\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.transformer = TFXLMMainLayer(config, name=\"transformer\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.transformer = TFXLMMainLayer(config, name=\"transformer\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.init_std), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.transformer = TFXLMMainLayer(config, name=\"transformer\")\n        self.sequence_summary = TFSequenceSummary(config, initializer_range=config.init_std, name=\"sequence_summary\")\n        self.logits_proj = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"logits_proj\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.d_model % config.n_head != 0:\n            raise ValueError(\n                f\"The hidden size ({config.d_model}) is not a multiple of the number of attention \"\n                f\"heads ({config.n_head}\"\n            )\n\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n        self.d_model = config.d_model\n        self.scale = 1 / (config.d_head ** 0.5)\n        self.initializer_range = config.initializer_range\n        self.output_attentions = config.output_attentions\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.d_model % config.n_head != 0:\n            raise ValueError(\n                f\"The hidden size ({config.d_model}) is not a multiple of the number of attention \"\n                f\"heads ({config.n_head}\"\n            )\n\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n        self.d_model = config.d_model\n        self.scale = 1 / (config.d_head ** 0.5)\n        self.initializer_range = config.initializer_range\n        self.output_attentions = config.output_attentions\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)\n            ef = tf.einsum(\"ijbs,ibns->ijbn\", seg_mat, ef)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)\n            ef = tf.einsum(\"ijbs,ibns->ijbn\", seg_mat, ef)\n\n        # merge attention scores and perform masking\n        attn_score = (ac + bd + ef) * self.scale\n        if attn_mask is not None:\n            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n            if attn_mask.dtype == tf.float16 or attn_mask.dtype == tf.bfloat16:\n                attn_score = attn_score - 65500 * attn_mask\n            else:\n                attn_score = attn_score - 1e30 * attn_mask\n\n        # attention probability\n        attn_prob = tf.nn.softmax(attn_score, axis=1)"}
{"text": "    def rel_attn_core(\n        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n    ):\n        \"\"\"Core relative positional attention operations.\"\"\"\n        # content based attention score\n        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)\n            ef = tf.einsum(\"ijbs,ibns->ijbn\", seg_mat, ef)\n\n        # merge attention scores and perform masking\n        attn_score = (ac + bd + ef) * self.scale\n        if attn_mask is not None:\n            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n            if attn_mask.dtype == tf.float16 or attn_mask.dtype == tf.bfloat16:\n                attn_score = attn_score - 65500 * attn_mask\n            else:\n                attn_score = attn_score - 1e30 * attn_mask\n\n        # attention probability\n        attn_prob = tf.nn.softmax(attn_score, axis=1)\n\n        attn_prob = self.dropout(attn_prob, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_prob = attn_prob * head_mask\n\n        # attention output\n        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, v_head_h)"}
{"text": "    def call(\n        self,\n        h,\n        g,\n        attn_mask_h,\n        attn_mask_g,\n        r,\n        seg_mat,\n        mems,\n        target_mapping,\n        head_mask,\n        output_attentions,\n        training=False,\n    ):\n        if g is not None:\n            # Two-stream attention with relative positional encoding.\n            # content based attention score\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)"}
{"text": "    def call(\n        self,\n        h,\n        g,\n        attn_mask_h,\n        attn_mask_g,\n        r,\n        seg_mat,\n        mems,\n        target_mapping,\n        head_mask,\n        output_attentions,\n        training=False,\n    ):\n        if g is not None:\n            # Two-stream attention with relative positional encoding.\n            # content based attention score\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content-based key head\n            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n\n            # content-based value head\n            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)"}
{"text": "    def call(\n        self,\n        h,\n        g,\n        attn_mask_h,\n        attn_mask_g,\n        r,\n        seg_mat,\n        mems,\n        target_mapping,\n        head_mask,\n        output_attentions,\n        training=False,\n    ):\n        if g is not None:\n            # Two-stream attention with relative positional encoding.\n            # content based attention score\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content-based key head\n            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n\n            # content-based value head\n            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n\n            # position-based key head\n            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)"}
{"text": "    def call(\n        self,\n        h,\n        g,\n        attn_mask_h,\n        attn_mask_g,\n        r,\n        seg_mat,\n        mems,\n        target_mapping,\n        head_mask,\n        output_attentions,\n        training=False,\n    ):\n        if g is not None:\n            # Two-stream attention with relative positional encoding.\n            # content based attention score\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content-based key head\n            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n\n            # content-based value head\n            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n\n            # position-based key head\n            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)\n\n            # h-stream\n            # content-stream query head\n            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)"}
{"text": "training=training,\n            )\n\n            if output_attentions:\n                attn_vec_h, attn_prob_h = attn_vec_h\n\n            # post processing\n            output_h = self.post_attention(h, attn_vec_h, training=training)\n\n            # g-stream\n            # query-stream query head\n            q_head_g = tf.einsum(\"ibh,hnd->ibnd\", g, self.q)\n\n            # core attention ops\n            if target_mapping is not None:\n                q_head_g = tf.einsum(\"mbnd,mlb->lbnd\", q_head_g, target_mapping)\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n                attn_vec_g = tf.einsum(\"lbnd,mlb->mbnd\", attn_vec_g, target_mapping)\n            else:\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n            # post processing\n            output_g = self.post_attention(g, attn_vec_g, training=training)\n\n            if output_attentions:\n                attn_prob = attn_prob_h, attn_prob_g\n\n        else:\n            # Multi-head attention with relative positional encoding\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content heads\n            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)"}
{"text": "ons:\n                attn_vec_h, attn_prob_h = attn_vec_h\n\n            # post processing\n            output_h = self.post_attention(h, attn_vec_h, training=training)\n\n            # g-stream\n            # query-stream query head\n            q_head_g = tf.einsum(\"ibh,hnd->ibnd\", g, self.q)\n\n            # core attention ops\n            if target_mapping is not None:\n                q_head_g = tf.einsum(\"mbnd,mlb->lbnd\", q_head_g, target_mapping)\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n                attn_vec_g = tf.einsum(\"lbnd,mlb->mbnd\", attn_vec_g, target_mapping)\n            else:\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n            # post processing\n            output_g = self.post_attention(g, attn_vec_g, training=training)\n\n            if output_attentions:\n                attn_prob = attn_prob_h, attn_prob_g\n\n        else:\n            # Multi-head attention with relative positional encoding\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content heads\n            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)"}
{"text": "        # post processing\n            output_h = self.post_attention(h, attn_vec_h, training=training)\n\n            # g-stream\n            # query-stream query head\n            q_head_g = tf.einsum(\"ibh,hnd->ibnd\", g, self.q)\n\n            # core attention ops\n            if target_mapping is not None:\n                q_head_g = tf.einsum(\"mbnd,mlb->lbnd\", q_head_g, target_mapping)\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n                attn_vec_g = tf.einsum(\"lbnd,mlb->mbnd\", attn_vec_g, target_mapping)\n            else:\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g,\n                    k_head_h,\n                    v_head_h,\n                    k_head_r,\n                    seg_mat,\n                    attn_mask_g,\n                    head_mask,\n                    output_attentions,\n                    training=training,\n                )\n\n                if output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n            # post processing\n            output_g = self.post_attention(g, attn_vec_g, training=training)\n\n            if output_attentions:\n                attn_prob = attn_prob_h, attn_prob_g\n\n        else:\n            # Multi-head attention with relative positional encoding\n            if mems is not None and len(shape_list(mems)) > 1:\n                cat = tf.concat([mems, h], axis=0)\n            else:\n                cat = h\n\n            # content heads\n            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)"}
{"text": "    def cache_mem(self, curr_out, prev_mem):\n        # cache hidden states into memory.\n        if self.reuse_len is not None and self.reuse_len > 0:\n            curr_out = curr_out[: self.reuse_len]\n\n        if self.mem_len is None or self.mem_len == 0:\n            # If :obj:`use_mems` is active but no `mem_len` is defined, the model behaves like GPT-2 at inference time\n            # and returns all of the past and current hidden states.\n            cutoff = 0\n        else:\n            # If :obj:`use_mems` is active and `mem_len` is defined, the model returns the last `mem_len` hidden\n            # states. This is the preferred setting for training and long-form generation.\n            cutoff = -self.mem_len\n        if prev_mem is None:\n            # if :obj:`use_mems` is active and `mem_len` is defined, the model\n            new_mem = curr_out[cutoff:]\n        else:\n            new_mem = tf.concat([prev_mem, curr_out], 0)"}
{"text": "    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        \"\"\"create relative positional encoding.\"\"\"\n        freq_seq = tf.range(0, self.d_model, 2.0)"}
{"text": "    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        \"\"\"create relative positional encoding.\"\"\"\n        freq_seq = tf.range(0, self.d_model, 2.0)\n        inv_freq = 1 / (10000 ** (freq_seq / self.d_model))\n\n        if self.attn_type == \"bi\":\n            # beg, end = klen - 1, -qlen\n            beg, end = klen, -qlen\n        elif self.attn_type == \"uni\":\n            # beg, end = klen - 1, -1\n            beg, end = klen, -1\n        else:\n            raise ValueError(f\"Unknown `attn_type` {self.attn_type}.\")\n\n        if self.bi_data:\n            fwd_pos_seq = tf.range(beg, end, -1.0)"}
{"text": "    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        \"\"\"create relative positional encoding.\"\"\"\n        freq_seq = tf.range(0, self.d_model, 2.0)\n        inv_freq = 1 / (10000 ** (freq_seq / self.d_model))\n\n        if self.attn_type == \"bi\":\n            # beg, end = klen - 1, -qlen\n            beg, end = klen, -qlen\n        elif self.attn_type == \"uni\":\n            # beg, end = klen - 1, -1\n            beg, end = klen, -1\n        else:\n            raise ValueError(f\"Unknown `attn_type` {self.attn_type}.\")\n\n        if self.bi_data:\n            fwd_pos_seq = tf.range(beg, end, -1.0)\n            bwd_pos_seq = tf.range(-beg, -end, 1.0)"}
{"text": "    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        \"\"\"create relative positional encoding.\"\"\"\n        freq_seq = tf.range(0, self.d_model, 2.0)\n        inv_freq = 1 / (10000 ** (freq_seq / self.d_model))\n\n        if self.attn_type == \"bi\":\n            # beg, end = klen - 1, -qlen\n            beg, end = klen, -qlen\n        elif self.attn_type == \"uni\":\n            # beg, end = klen - 1, -1\n            beg, end = klen, -1\n        else:\n            raise ValueError(f\"Unknown `attn_type` {self.attn_type}.\")\n\n        if self.bi_data:\n            fwd_pos_seq = tf.range(beg, end, -1.0)\n            bwd_pos_seq = tf.range(-beg, -end, 1.0)\n\n            if self.clamp_len > 0:\n                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)\n                bwd_pos_seq = tf.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)\n\n            if bsz is not None:\n                if bsz % 2 != 0:\n                    raise ValueError(f\"With bi_data, the batch size {bsz} should be divisible by 2\")\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n            else:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n\n            pos_emb = tf.concat([fwd_pos_emb, bwd_pos_emb], axis=1)"}
{"text": "=(1, 0)) if inputs[\"attention_mask\"] is not None else None\n        )\n        inputs[\"perm_mask\"] = (\n            tf.transpose(inputs[\"perm_mask\"], perm=(1, 2, 0)) if inputs[\"perm_mask\"] is not None else None\n        )\n        inputs[\"target_mapping\"] = (\n            tf.transpose(inputs[\"target_mapping\"], perm=(1, 2, 0)) if inputs[\"target_mapping\"] is not None else None\n        )\n\n        mlen = shape_list(inputs[\"mems\"][0])[0] if inputs[\"mems\"] is not None and inputs[\"mems\"][0] is not None else 0\n        klen = mlen + qlen\n\n        # Attention mask\n        # causal attention mask\n        if self.attn_type == \"uni\":\n            attn_mask = self.create_mask(qlen, mlen)\n            attn_mask = attn_mask[:, :, None, None]\n        elif self.attn_type == \"bi\":\n            attn_mask = None\n        else:\n            raise ValueError(f\"Unsupported attention type: {self.attn_type}\")\n\n        # data mask: input mask & perm mask\n        assert inputs[\"input_mask\"] is None or inputs[\"attention_mask\"] is None, (\n            \"You can only use one of input_mask (uses 1 for padding) \"\n            \"or attention_mask (uses 0 for padding, added for compatibility with BERT). Please choose one.\"\n        )\n        if inputs[\"input_mask\"] is None and inputs[\"attention_mask\"] is not None:\n            one_cst = tf.constant(1.0)\n            inputs[\"input_mask\"] = 1.0 - tf.cast(inputs[\"attention_mask\"], dtype=one_cst.dtype)\n        if inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is not None:\n            data_mask = inputs[\"input_mask\"][None] + inputs[\"perm_mask\"]\n        elif inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is None:\n            data_mask = inputs[\"input_mask\"][None]\n        elif inputs[\"input_mask\"] is None and inputs[\"perm_mask\"] is not None:\n            data_mask = inputs[\"perm_mask\"]\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            if mlen > 0:\n                mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz])"}
{"text": "nput_mask\"] is None and inputs[\"perm_mask\"] is not None:\n            data_mask = inputs[\"perm_mask\"]\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            if mlen > 0:\n                mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz])\n                data_mask = tf.concat([mems_mask, data_mask], axis=1)\n            if attn_mask is None:\n                attn_mask = data_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = tf.cast(attn_mask > 0, dtype=attn_mask.dtype)\n\n        if attn_mask is not None:\n            non_tgt_mask = -tf.eye(qlen)\n            if mlen > 0:\n                non_tgt_mask = tf.concat([tf.zeros([qlen, mlen]), non_tgt_mask], axis=-1)\n            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=non_tgt_mask.dtype)\n        else:\n            non_tgt_mask = None\n\n        # Word embeddings and prepare h & g hidden states\n        if inputs[\"inputs_embeds\"] is not None:\n            word_emb_k = inputs[\"inputs_embeds\"]\n        else:\n            word_emb_k = self.word_embedding(inputs[\"input_ids\"])\n        output_h = self.dropout(word_emb_k, training=inputs[\"training\"])\n        if inputs[\"target_mapping\"] is not None:\n            word_emb_q = tf.tile(self.mask_emb, [shape_list(inputs[\"target_mapping\"])[0], bsz, 1])\n            # else:  # We removed the inp_q input which was same as target mapping\n            #     inp_q_ext = inp_q[:, :, None]\n            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n            output_g = self.dropout(word_emb_q, training=inputs[\"training\"])\n        else:\n            output_g = None\n\n        # Segment embedding\n        if inputs[\"token_type_ids\"] is not None:\n            # Convert `token_type_ids` to one-hot `seg_mat`\n            if mlen > 0:\n                mem_pad = tf.zeros([mlen, bsz], dtype=inputs[\"token_type_ids\"].dtype)"}
{"text": "sk = inputs[\"perm_mask\"]\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            if mlen > 0:\n                mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz])\n                data_mask = tf.concat([mems_mask, data_mask], axis=1)\n            if attn_mask is None:\n                attn_mask = data_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = tf.cast(attn_mask > 0, dtype=attn_mask.dtype)\n\n        if attn_mask is not None:\n            non_tgt_mask = -tf.eye(qlen)\n            if mlen > 0:\n                non_tgt_mask = tf.concat([tf.zeros([qlen, mlen]), non_tgt_mask], axis=-1)\n            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=non_tgt_mask.dtype)\n        else:\n            non_tgt_mask = None\n\n        # Word embeddings and prepare h & g hidden states\n        if inputs[\"inputs_embeds\"] is not None:\n            word_emb_k = inputs[\"inputs_embeds\"]\n        else:\n            word_emb_k = self.word_embedding(inputs[\"input_ids\"])\n        output_h = self.dropout(word_emb_k, training=inputs[\"training\"])\n        if inputs[\"target_mapping\"] is not None:\n            word_emb_q = tf.tile(self.mask_emb, [shape_list(inputs[\"target_mapping\"])[0], bsz, 1])\n            # else:  # We removed the inp_q input which was same as target mapping\n            #     inp_q_ext = inp_q[:, :, None]\n            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n            output_g = self.dropout(word_emb_q, training=inputs[\"training\"])\n        else:\n            output_g = None\n\n        # Segment embedding\n        if inputs[\"token_type_ids\"] is not None:\n            # Convert `token_type_ids` to one-hot `seg_mat`\n            if mlen > 0:\n                mem_pad = tf.zeros([mlen, bsz], dtype=inputs[\"token_type_ids\"].dtype)\n                cat_ids = tf.concat([mem_pad, inputs[\"token_type_ids\"]], 0)"}
{"text": "a_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = tf.cast(attn_mask > 0, dtype=attn_mask.dtype)\n\n        if attn_mask is not None:\n            non_tgt_mask = -tf.eye(qlen)\n            if mlen > 0:\n                non_tgt_mask = tf.concat([tf.zeros([qlen, mlen]), non_tgt_mask], axis=-1)\n            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=non_tgt_mask.dtype)\n        else:\n            non_tgt_mask = None\n\n        # Word embeddings and prepare h & g hidden states\n        if inputs[\"inputs_embeds\"] is not None:\n            word_emb_k = inputs[\"inputs_embeds\"]\n        else:\n            word_emb_k = self.word_embedding(inputs[\"input_ids\"])\n        output_h = self.dropout(word_emb_k, training=inputs[\"training\"])\n        if inputs[\"target_mapping\"] is not None:\n            word_emb_q = tf.tile(self.mask_emb, [shape_list(inputs[\"target_mapping\"])[0], bsz, 1])\n            # else:  # We removed the inp_q input which was same as target mapping\n            #     inp_q_ext = inp_q[:, :, None]\n            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n            output_g = self.dropout(word_emb_q, training=inputs[\"training\"])\n        else:\n            output_g = None\n\n        # Segment embedding\n        if inputs[\"token_type_ids\"] is not None:\n            # Convert `token_type_ids` to one-hot `seg_mat`\n            if mlen > 0:\n                mem_pad = tf.zeros([mlen, bsz], dtype=inputs[\"token_type_ids\"].dtype)\n                cat_ids = tf.concat([mem_pad, inputs[\"token_type_ids\"]], 0)\n            else:\n                cat_ids = inputs[\"token_type_ids\"]\n\n            # `1` indicates not in the same segment [qlen x klen x bsz]\n            seg_mat = tf.cast(\n                tf.logical_not(tf.equal(inputs[\"token_type_ids\"][:, None], cat_ids[None, :])),\n                dtype=inputs[\"token_type_ids\"].dtype,\n            )\n            seg_mat = tf.one_hot(seg_mat, 2)"}
