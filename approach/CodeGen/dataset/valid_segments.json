{"text": "tates, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])\n        conv_kernel_layer = tf.nn.softmax(conv_kernel_layer, axis=1)\n\n        paddings = tf.constant(\n            [\n                [\n                    0,\n                    0,\n                ],\n                [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)],\n                [0, 0],\n            ]\n        )\n\n        conv_out_layer = self.conv_out_layer(hidden_states)\n        conv_out_layer = tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])\n        conv_out_layer = tf.pad(conv_out_layer, paddings, \"CONSTANT\")\n\n        unfold_conv_out_layer = tf.stack(\n            [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )"}
{"text": "tf.reshape(conv_out_layer, [batch_size, -1, self.all_head_size])\n        conv_out_layer = tf.pad(conv_out_layer, paddings, \"CONSTANT\")\n\n        unfold_conv_out_layer = tf.stack(\n            [\n                tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size])\n                for i in range(self.conv_kernel_size)\n            ],\n            axis=-1,\n        )\n\n        conv_out_layer = tf.reshape(unfold_conv_out_layer, [-1, self.attention_head_size, self.conv_kernel_size])\n\n        conv_out_layer = tf.matmul(conv_out_layer, conv_kernel_layer)\n        conv_out_layer = tf.reshape(conv_out_layer, [-1, self.all_head_size])\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n        attention_scores = attention_scores / tf.math.sqrt(dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs, training=training)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        value_layer = tf.reshape(\n            mixed_value_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size]\n        )\n        value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n        context_layer = tf.matmul(attention_probs, value_layer)"}
{"text": "    def call(\n        self,\n        input_tensor: tf.Tensor,\n        attention_mask: tf.Tensor,\n        head_mask: tf.Tensor,\n        output_attentions: bool,\n        training: bool = False,\n    ) -> Tuple[tf.Tensor]:\n        batch_size = shape_list(input_tensor)[0]\n        mixed_query_layer = self.query(inputs=input_tensor)\n        mixed_key_layer = self.key(inputs=input_tensor)\n        mixed_value_layer = self.value(inputs=input_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        # (batch size, num_heads, seq_len_q, seq_len_k)\n        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n        attention_scores = tf.divide(attention_scores, dk)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in TFAlbertModel call() function)\n            attention_scores = tf.add(attention_scores, attention_mask)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: RoFormerConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.roformer = TFRoFormerMainLayer(config, name=\"roformer\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: LayoutLMConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "    def __init__(self, config: RoFormerConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.roformer = TFRoFormerMainLayer(config, name=\"roformer\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n        super().__init__(**kwargs)\n        self.SelfAttention = TFT5Attention(\n            config,\n            has_relative_attention_bias=has_relative_attention_bias,\n            name=\"SelfAttention\",\n        )\n        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.config = config\n        self.is_decoder = config.is_decoder\n\n        self.embeddings = TFElectraEmbeddings(config, name=\"embeddings\")\n\n        if config.embedding_size != config.hidden_size:\n            self.embeddings_project = tf.keras.layers.Dense(config.hidden_size, name=\"embeddings_project\")"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.output_attentions = config.output_attentions\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n        )\n        self.key = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n        )"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "    def __init__(self, config: LayoutLMConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.layoutlm = TFLayoutLMMainLayer(config, name=\"layoutlm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        classifier_dropout = (\n            config.classifhidden_dropout_probier_dropout\n            if config.classifier_dropout is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = tf.keras.layers.Dropout(classifier_dropout)\n        self.out_proj = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"out_proj\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n        self.convbert = TFConvBertMainLayer(config, name=\"convbert\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def call(\n        self, query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions, training=False\n    ):\n        batch_size = shape_list(attention_mask)[0]\n        mixed_query_layer = self.query(query_tensor)\n        mixed_key_layer = self.key(key_tensor)\n        mixed_value_layer = self.value(value_tensor)\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = tf.matmul(\n            query_layer, key_layer, transpose_b=True\n        )"}
{"text": "    def __init__(self, config, layer_id, **kwargs):\n        super().__init__(**kwargs)\n        self.longformer_self_attn = TFLEDEncoderSelfAttention(config, layer_id=layer_id, name=\"longformer_self_attn\")\n        self.output_dense = tf.keras.layers.Dense(config.d_model, use_bias=True, name=\"output\")"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n\n        if mems is not None:\n            mems = tf.cast(mems, dtype=w.dtype)\n            cat = tf.concat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)"}
{"text": "    def call(self, v, k, q, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n        batch_size = shape_list(q)[0]\n\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, batch_size)\n        k = self.split_into_heads(k, batch_size)\n        v = self.split_into_heads(v, batch_size)\n\n        if layer_past is not None:\n            past_key, past_value = tf.unstack(layer_past, axis=0)\n            k = tf.concat((past_key, k), axis=-2)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "l cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + tf.cast(\n                attention_mask, dtype=attn_weights.dtype\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "    def __init__(self, config: LEDConfig, layer_id: int, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFLEDEncoderAttention(config, layer_id, name=\"self_attn\")\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "eds(self, seq_len, training=False):\n        \"\"\"\n        Create and cache inputs related to relative position encoding. Those are very different depending on whether we\n        are using the factorized or the relative shift attention:\n\n        For the factorized attention, it returns the matrices (phi, pi, psi, omega) used in the paper, appendix A.2.2,\n        final formula.\n\n        For the relative shift attention, it returns all possible vectors R used in the paper, appendix A.2.1, final\n        formula.\n\n        Paper link: https://arxiv.org/abs/2006.03236\n        \"\"\"\n        if self.attention_type == \"factorized\":\n            # Notations from the paper, appending A.2.2, final formula.\n            # We need to create and return the matrices phi, psi, pi and omega.\n            pos_seq = tf.range(0, seq_len, 1.0)\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            sinusoid = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n\n            sin_embed = tf.sin(sinusoid)\n            sin_embed_d = self.sin_dropout(sin_embed, training=training)\n            cos_embed = tf.cos(sinusoid)\n            cos_embed_d = self.cos_dropout(cos_embed, training=training)\n            # This is different from the formula on the paper...\n            phi = tf.concat([sin_embed_d, sin_embed_d], axis=-1)\n            psi = tf.concat([cos_embed, sin_embed], axis=-1)\n            pi = tf.concat([cos_embed_d, cos_embed_d], axis=-1)\n            omega = tf.concat([-sin_embed, cos_embed], axis=-1)\n            return (phi, pi, psi, omega)\n        else:\n            # Notations from the paper, appending A.2.1, final formula.\n            # We need to create and return all the possible vectors R for all blocks and shifts.\n            freq_seq = tf.range(0, self.d_model // 2, 1.0)\n            inv_freq = 1 / (10000 ** (freq_seq / (self.d_model // 2)))\n            # Maximum relative positions for the first input\n            rel_pos_id = tf.range(-seq_len * 2, seq_len * 2, 1.0)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.q = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"q\"\n        )\n        self.k = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"k\"\n        )\n        self.v = tf.keras.layers.Dense(\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"v\"\n        )"}
{"text": "    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")"}
{"text": "    def __init__(self, config: ViTConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )"}
{"text": "def build_relative_position(query_size, key_size):\n    \"\"\"\n    Build relative position according to the query and key\n\n    We assume the absolute position of query :math:`P_q` is range from (0, query_size) and the absolute position of key\n    :math:`P_k` is range from (0, key_size), The relative positions from query to key is :math:`R_{q \\\\rightarrow k} =\n    P_q - P_k`\n\n    Args:\n        query_size (int): the length of query\n        key_size (int): the length of key\n\n    Return:\n        :obj:`tf.Tensor`: A tensor with shape [1, query_size, key_size]\n\n    \"\"\"\n    q_ids = tf.range(query_size, dtype=tf.int32)\n    k_ids = tf.range(key_size, dtype=tf.int32)"}
{"text": "    def compute_position_bias(self, x, position_ids=None):\n        \"\"\"Compute binned relative position bias\"\"\"\n        input_shape = shape_list(x)\n        qlen, klen = input_shape[1], input_shape[1]\n\n        if position_ids is not None:\n            context_position = position_ids[:, :, None]\n            memory_position = position_ids[:, None, :]\n        else:\n            context_position = tf.range(qlen)[:, None]\n            memory_position = tf.range(klen)"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)"}
{"text": "    def __init__(self, config: LongformerConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"tanh\",\n            name=\"dense\",\n        )"}
{"text": "    def call(self, query, key, value, attention_inputs, output_attentions=False, training=False):\n        # query has shape batch_size x seq_len x d_model\n        # key and value have shapes batch_size x context_len x d_model\n        position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs\n\n        batch_size, seq_len, _ = shape_list(query)\n        context_len = shape_list(key)[1]\n        n_head, d_head = self.n_head, self.d_head\n\n        # Shape batch_size x seq_len x n_head x d_head\n        q_head = tf.reshape(self.q_head(query), [batch_size, seq_len, n_head, d_head])\n        # Shapes batch_size x context_len x n_head x d_head\n        k_head = tf.reshape(self.k_head(key), [batch_size, context_len, n_head, d_head])\n        v_head = tf.reshape(self.v_head(value), [batch_size, context_len, n_head, d_head])\n\n        q_head = q_head * self.scale\n        # Shape n_head x d_head\n        r_w_bias = self.r_w_bias * self.scale\n        # Shapes batch_size x n_head x seq_len x context_len\n        content_score = tf.einsum(\"bind,bjnd->bnij\", q_head + r_w_bias, k_head)"}
{"text": "    def call(\n        self,\n        input_ids: Optional[TFModelInputType] = None,\n        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(dims=input_shape, value=1)\n\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: DebertaConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.num_labels = config.num_labels\n\n        self.deberta = TFDebertaMainLayer(config, name=\"deberta\")\n        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n        self.classifier = tf.keras.layers.Dense(\n            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.mpnet = TFMPNetMainLayer(config, name=\"mpnet\")\n        self.qa_outputs = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )"}
{"text": "    def post_attention(self, h, attn_vec, residual=True, training=False):\n        \"\"\"Post-attention processing.\"\"\"\n        # post-attention projection (back to `d_model`)\n        attn_out = tf.einsum(\"ibnd,hnd->ibh\", attn_vec, self.o)"}
{"text": "    def __init__(self, config, n_labels, **kwargs):\n        super().__init__(**kwargs)\n        initializer = get_initializer(config.initializer_range)\n        self.linear_hidden = tf.keras.layers.Dense(\n            config.d_model, kernel_initializer=initializer, name=\"linear_hidden\"\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.linear_out = tf.keras.layers.Dense(n_labels, kernel_initializer=initializer, name=\"linear_out\")"}
{"text": "    def call(self, hidden, training=False):\n        hidden = self.linear_hidden(hidden)\n        hidden = tf.keras.activations.tanh(hidden)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.longformer = TFLongformerMainLayer(config, name=\"longformer\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n\n        self.funnel = TFFunnelMainLayer(config, name=\"funnel\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.classifier = tf.keras.layers.Dense(\n            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"query\",\n        )\n        self.key = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"key\",\n        )\n        self.value = tf.keras.layers.Dense(\n            self.all_head_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"value\",\n        )\n\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)"}
{"text": "    def __init__(self, nx, config, scale=False, is_cross_attention=False, **kwargs):\n        super().__init__(**kwargs)\n\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implementation]\n        assert n_state % config.n_head == 0\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.output_attentions = config.output_attentions\n\n        self.is_cross_attention = is_cross_attention\n\n        if self.is_cross_attention:\n            self.c_attn = TFConv1D(n_state * 2, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n            self.q_attn = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"q_attn\")\n        else:\n            self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n\n        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "          # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)"}
{"text": "    def __init__(self, config, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(\n            config.hidden_size,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"dense\",\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"}
{"text": "    def marginalize(self, seq_logits, doc_scores, n_docs=None):\n        n_docs = n_docs if n_docs is not None else self.config.n_docs\n\n        # RAG-token marginalization\n        seq_logprobs = tf.nn.log_softmax(seq_logits, axis=-1)\n        seq_logprobs = tf.reshape(seq_logprobs, [seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.shape[-1]])\n        doc_logprobs = tf.nn.log_softmax(doc_scores, axis=1)"}
{"text": "    def __init__(self, config: DPRConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.encoder = TFDPREncoderLayer(config, name=\"encoder\")\n\n        self.qa_outputs = tf.keras.layers.Dense(\n            2, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n        )\n        self.qa_classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_classifier\"\n        )"}
{"text": "    def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.embed_tokens = embed_tokens\n        self.layerdrop = config.decoder_layerdrop\n        self.embed_positions = TFBlenderbotLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n            name=\"embed_positions\",\n        )\n        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n        self.layers = [TFBlenderbotDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        head_mask=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.ones(input_shape)"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        self.vocab_size = config.vocab_size\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.vocab_transform = tf.keras.layers.Dense(\n            config.dim, kernel_initializer=get_initializer(config.initializer_range), name=\"vocab_transform\"\n        )"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)"}
{"text": "    def __init__(self, config, num_labels, **kwargs):\n        super().__init__(**kwargs)\n        hid_dim = config.hidden_size\n        self.dense = tf.keras.layers.Dense(\n            hid_dim * 2,\n            kernel_initializer=get_initializer(config.initializer_range),\n            name=\"logit_fc_._0\",\n        )\n        self.activation = get_tf_activation(\"gelu\")\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"logit_fc_._2\")"}
{"text": "    def call(\n        self,\n        input_ids=None,\n        visual_feats=None,\n        visual_pos=None,\n        attention_mask=None,\n        visual_attention_mask=None,\n        token_type_ids=None,\n        inputs_embeds=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        training=False,\n        **kwargs,\n    ):\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            visual_feats=visual_feats,\n            visual_pos=visual_pos,\n            attention_mask=attention_mask,\n            visual_attention_mask=visual_attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n        if inputs[\"visual_pos\"] is None or inputs[\"visual_feats\"] is None:\n            raise ValueError(\"visual_feats and visual_pos cannot be `None` in LXMERT's `call` method.\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.fill(input_shape, 1)"}
{"text": " all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n        key_states = tf.reshape(key_states, proj_shape)\n        value_states = tf.reshape(value_states, proj_shape)\n\n        src_len = shape_list(key_states)[1]\n        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n\n        # The tf.debugging asserts are not compliant with XLA then they\n        # have to be disabled in other modes than eager.\n        if tf.executing_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)"}
{"text": "ng_eagerly():\n            tf.debugging.assert_equal(\n                shape_list(attn_weights),\n                [bsz * self.num_heads, tgt_len, src_len],\n                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n            )\n\n        if attention_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(attention_mask),\n                    [bsz, 1, tgt_len, src_len],\n                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n                )\n\n            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        if layer_head_mask is not None:\n            # The tf.debugging asserts are not compliant with XLA then they\n            # have to be disabled in other modes than eager.\n            if tf.executing_eagerly():\n                tf.debugging.assert_equal(\n                    shape_list(layer_head_mask),\n                    [self.num_heads],\n                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n                )\n\n            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n            )\n            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n\n        attn_probs = self.dropout(attn_weights, training=training)\n        attn_output = tf.matmul(attn_probs, value_states)"}
{"text": "    def __init__(self, config: RemBertConfig, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense = tf.keras.layers.Dense(\n            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n        )\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")"}
{"text": "=None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFDPRContextEncoderOutput, Tuple[tf.Tensor, ...]]:\n        r\"\"\"\n        Return:\n\n        Examples::\n\n            >>> from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer\n            >>> tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n            >>> model = TFDPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', from_pt=True)\n            >>> input_ids = tokenizer(\"Hello, is my dog cute ?\", return_tensors='tf')[\"input_ids\"]\n            >>> embeddings = model(input_ids).pooler_output\n        \"\"\"\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = (\n                tf.ones(input_shape, dtype=tf.dtypes.int32)\n                if inputs[\"input_ids\"] is None\n                else (inputs[\"input_ids\"] != self.config.pad_token_id)\n            )\n        if inputs[\"token_type_ids\"] is None:\n            inputs[\"token_type_ids\"] = tf.zeros(input_shape, dtype=tf.dtypes.int32)"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")"}
{"text": "    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.distilbert = TFDistilBertMainLayer(config, name=\"distilbert\")\n        self.dropout = tf.keras.layers.Dropout(config.seq_classif_dropout)\n        self.pre_classifier = tf.keras.layers.Dense(\n            config.dim,\n            kernel_initializer=get_initializer(config.initializer_range),\n            activation=\"relu\",\n            name=\"pre_classifier\",\n        )\n        self.classifier = tf.keras.layers.Dense(\n            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n        )"}
{"text": "    def __init__(self, config: BlenderbotSmallConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFBlenderbotSmallAttention(\n            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n        )\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")"}
{"text": "    def call(\n        self,\n        hidden_states: tf.Tensor,\n        key_value_states: Optional[tf.Tensor] = None,\n        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n        attention_mask: Optional[tf.Tensor] = None,\n        layer_head_mask: Optional[tf.Tensor] = None,\n        training=False,\n    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n            value_states = tf.concat([past_key_value[1], value_states], axis=2)"}
{"text": "    def call(\n        self,\n        input_ids: tf.Tensor = None,\n        position_ids: tf.Tensor = None,\n        token_type_ids: tf.Tensor = None,\n        inputs_embeds: tf.Tensor = None,\n        past_key_values_length=0,\n        training: bool = False,\n    ) -> tf.Tensor:\n        \"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"\n        if input_ids is None and inputs_embeds is None:\n            raise ValueError(\"Need to provide either `input_ids` or `input_embeds`.\")\n\n        if input_ids is not None:\n            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n\n        input_shape = shape_list(inputs_embeds)[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = tf.fill(dims=input_shape, value=0)"}
{"text": "    def __init__(self, config: HubertConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = TFHubertAttention(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=False,\n            name=\"attention\",\n        )\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout)\n        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")"}
{"text": "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n        batch_size = shape_list(hidden_states)[0]\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n        conv_attn_layer = tf.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n\n        conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)\n        conv_kernel_layer = tf.reshape(conv_kernel_layer, [-1, self.conv_kernel_size, 1])"}
{"text": ",\n        return_dict=None,\n        training: bool = False,\n        **kwargs,\n    ) -> Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]:\n        r\"\"\"\n        Return:\n\n        Examples::\n\n            >>> from transformers import TFDPRReader, DPRReaderTokenizer\n            >>> tokenizer = DPRReaderTokenizer.from_pretrained('facebook/dpr-reader-single-nq-base')\n            >>> model = TFDPRReader.from_pretrained('facebook/dpr-reader-single-nq-base', from_pt=True)\n            >>> encoded_inputs = tokenizer(\n            ...         questions=[\"What is love ?\"],\n            ...         titles=[\"Haddaway\"],\n            ...         texts=[\"'What Is Love' is a song recorded by the artist Haddaway\"],\n            ...         return_tensors='tf'\n            ...     )\n            >>> outputs = model(encoded_inputs)\n            >>> start_logits = outputs.start_logits\n            >>> end_logits = outputs.end_logits\n            >>> relevance_logits = outputs.relevance_logits\n\n        \"\"\"\n        inputs = input_processing(\n            func=self.call,\n            config=self.config,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            training=training,\n            kwargs_call=kwargs,\n        )\n\n        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif inputs[\"input_ids\"] is not None:\n            input_shape = shape_list(inputs[\"input_ids\"])\n        elif inputs[\"inputs_embeds\"] is not None:\n            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs[\"attention_mask\"] is None:\n            inputs[\"attention_mask\"] = tf.ones(input_shape, dtype=tf.dtypes.int32)"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFMarianAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")"}
{"text": "    def __init__(self, config: MarianConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = config.d_model\n        self.self_attn = TFMarianAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"self_attn\",\n            is_decoder=True,\n        )\n        self.dropout = tf.keras.layers.Dropout(config.dropout)\n        self.activation_fn = get_tf_activation(config.activation_function)\n        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n\n        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n        self.encoder_attn = TFMarianAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            name=\"encoder_attn\",\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")"}
